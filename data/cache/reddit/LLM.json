{
  "keyword": "LLM",
  "platform": "reddit",
  "data_source": "time_all",
  "collection_time": "2025-10-07T14:45:05.246247",
  "posts": [
    {
      "platform": "reddit",
      "post_id": "1mt7ofk",
      "title": "Have LLM‚Äôs hit a technological wall?",
      "content": "It sure seems like it with the consensus that ChatGPT-5, while an improvement, was way overhyped and not the sort of leap forward needed to achieve AGI. Here‚Äôs what AI expert Gary Marcus has to say about it. What do you think? https://open.substack.com/pub/garymarcus/p/openais-waterloo?r=5ajobw&amp;utm_medium=ios",
      "author": "egghutt",
      "timestamp": "2025-08-18T08:44:30",
      "url": "https://reddit.com/r/ArtificialInteligence/comments/1mt7ofk/have_llms_hit_a_technological_wall/",
      "metadata": {
        "domain": "self.ArtificialInteligence",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ArtificialInteligence",
      "score": 120,
      "num_comments": 137,
      "upvote_ratio": 0.8,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1l5wxoa",
      "title": "My 160GB local LLM rig",
      "content": "Built this monster with 4x V100 and 4x 3090, with the threadripper / 256 GB RAM and 4x PSU. One Psu for power everything in the machine and 3x PSU 1000w to feed the beasts. Used bifurcated PCIE raisers to split out x16 PCIE to 4x x4 PCIEs. Ask me anything, biggest model I was able to run on this beast was qwen3 235B Q4 at around ~15 tokens / sec. Regularly I am running Devstral, qwen3 32B, gamma 3-27B, qwen3 4b x 3‚Ä¶.all in Q4 and use async to use all the models at the same time for different tasks.",
      "author": "TrifleHopeful5418",
      "timestamp": "2025-06-08T06:26:06",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1l5wxoa/my_160gb_local_llm_rig/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1365,
      "num_comments": 270,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1l6d6jz",
      "title": "It feels like LLM development has come to a dead-end.",
      "content": "(Currently, I'm using Snowpiercer 15b or Gemini 2.5 flash.) Somehow, it feels like people are just re-wrapping the same old datasets under a new name, with differences being marginal at best. Especially when it comes to smaller models between 12\\~22b. I've downloaded hundreds of models (with slight exaggeration) in the last 2 years, upgrading my rig just so I can run bigger LLMs. But I don't feel much of a difference other than the slight increase in the maximum size of context memory tokens. (Let's face it, they promote with 128k tokens, but all the existing LLMs look like they suffer from demantia at over 30k tokens.) The responses are still mostly uncreative, illogical and incoherent, so it feels less like an actual chat with an AI but more like a gacha where I have to heavily influence the result and make many edits to make anything interesting happen. LLMs seem incapable of handling more than a couple characters, and relationships always blur and bleed into each other. Nobody remembers anything, everything is so random. I feel disillusioned. Maybe LLMs are just overrated, and their design is fundamentally flawed. Am I wrong? Am I missing something here?",
      "author": "StudentFew6429",
      "timestamp": "2025-06-08T22:15:35",
      "url": "https://reddit.com/r/SillyTavernAI/comments/1l6d6jz/it_feels_like_llm_development_has_come_to_a/",
      "metadata": {
        "domain": "self.SillyTavernAI",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "SillyTavernAI",
      "score": 235,
      "num_comments": 131,
      "upvote_ratio": 0.89,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1ms4n55",
      "title": "What does it feel like: Cloud LLM vs Local LLM.",
      "content": "Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... üòÇ",
      "author": "Cool-Chemical-5629",
      "timestamp": "2025-08-17T03:10:29",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 606,
      "num_comments": 206,
      "upvote_ratio": 0.82,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mg2frk",
      "title": "Don‚Äôt understand LLM",
      "content": "I don‚Äôt understand LLM degrees. It seems to be extremely common for Europeans and those outside of the U.S. to have them, but here they are not common. It seems here you need a JD to get an LLM, but this is not the case in other countries. Is the LLM a terminal degree in other countries? Is it not just for a speciality like it is here? Can‚Äôt you practice without it in other countries?",
      "author": "Flashy-Actuator-998",
      "timestamp": "2025-08-03T05:56:07",
      "url": "https://reddit.com/r/LawSchool/comments/1mg2frk/dont_understand_llm/",
      "metadata": {
        "domain": "self.LawSchool",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LawSchool",
      "score": 68,
      "num_comments": 41,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lbd2jy",
      "title": "What LLM is everyone using in June 2025?",
      "content": "Curious what everyone‚Äôs running now. What model(s) are in your regular rotation? What hardware are you on? How are you running it? (LM Studio, Ollama, llama.cpp, etc.) What do you use it for? Here‚Äôs mine: Recently I've been using mostly Qwen3 (30B, 32B, and 235B) Ryzen 7 5800X, 128GB RAM, RTX 3090 Ollama + Open WebUI Mostly general use and private conversations I‚Äôd rather not run on cloud platforms",
      "author": "1BlueSpork",
      "timestamp": "2025-06-15T00:43:01",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 175,
      "num_comments": 123,
      "upvote_ratio": 0.91,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lux7bt",
      "title": "Saved $100k per year by explaining how AI/LLM work.",
      "content": "I work in a data science field, and I bring this up because I think it's data science related. We have an internal website that is very bare bones. It's made to be simplistic, because it's the reference document for our end-users (1000 of them) use. Executives heard about a software that would be completely AI driven, build detailed statistical insights, and change the world as they know it. I had a demo with the company and they explained its RAG capabilities, but mentioned it doesn't really \"learn\" like the assumption AI does. Our repo is so small and not at all needed for AI. We have used a fuzzy search that has worked for the past three years. Additionally, I have already built out dashboards that retrieve all the information executives have asked for via API (who's viewing pages, what are they searching, etc.) I showed the c-suite executives our current dashboards in Tableau, and how the actual search works. I also explained what RAG is, and how AI/LLMs work at a high level. I explained to them that AI is a fantastic tool, but I'm not sure if we should be spending 100k a year on it. They also asked if I have built any predictive models. I don't think they quite understood what that was as well, because we don't have the amount of data or need to predict anything. Needless to say, they decided it was best not to move forward \"for now\". I am shocked, but also not, that executives want to change the structure of how my team and end-users digest information just because they heard \"AI is awesome!\" They had zero idea how anything works in our shop. Oh yeah, our company has already laid of 250 people this year due to \"financial turbulence\", and now they're wanting to spend 100k on this?! It just goes to show you how deep the AI train runs. Did I handle this correctly and can I put this on my resume? LOL",
      "author": "tits_mcgee_92",
      "timestamp": "2025-07-09T03:03:19",
      "url": "https://reddit.com/r/datascience/comments/1lux7bt/saved_100k_per_year_by_explaining_how_aillm_work/",
      "metadata": {
        "domain": "self.datascience",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "datascience",
      "score": 1169,
      "num_comments": 98,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kn0wjm",
      "title": "Is anyone actually using LLM/AI tools at their real job in a meaningful way?",
      "content": "I work as a SWE at one of the \"tier 1\" tech companies in the Bay Area. I have noticed a huge disconnect between the cacophony of AI/LLM/vibecoding hype on social media, versus what I see at my job. Basically, as far as I can tell, nobody at work uses AI for anything work-related. We have access to a company-vetted IDE and ChatGPT style chatbot UI that uses SOTA models. The devprod group that produces these tools keeps diligently pushing people to try it, makes guides, info sessions etc. However, it's just not picking up (again, as far as I can tell). I suspect, then, that one of these 3 scenarios are playing out: 1. Devs at my company are secretly using AI tools and I'm just not in on it, due to some stigma or other reasons. 2. Devs at other companies are using AI but not at my company, due to deficiencies in my company's AI tooling or internal evangelism. 3. Practically no devs in the industry are using AI in a meaningful way. Do you use AI at work and how exactly?",
      "author": "WagwanKenobi",
      "timestamp": "2025-05-15T13:54:43",
      "url": "https://reddit.com/r/ExperiencedDevs/comments/1kn0wjm/is_anyone_actually_using_llmai_tools_at_their/",
      "metadata": {
        "domain": "self.ExperiencedDevs",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ExperiencedDevs",
      "score": 281,
      "num_comments": 449,
      "upvote_ratio": 0.87,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1n4y2jq",
      "title": "Your LLM setup",
      "content": "I'm planning a home lab build and I'm struggling to decide between paying extra for a GPU to run a small LLM locally or using one remotely (through openrouter for example). Those of you who have a remote LLM integrated into your Home Assistant, what service and LLM do you use, what is performance like (latency, accuracy, etc.), and how much does it cost you on average monthly?",
      "author": "LawlsMcPasta",
      "timestamp": "2025-08-31T23:45:19",
      "url": "https://reddit.com/r/homeassistant/comments/1n4y2jq/your_llm_setup/",
      "metadata": {
        "domain": "self.homeassistant",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "homeassistant",
      "score": 73,
      "num_comments": 75,
      "upvote_ratio": 0.83,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lph4vo",
      "title": "What locally hosted LLM did YOU choose and why?",
      "content": "Obviously, your end choice is highly dependent on your system capabilities and your intended use, but why did YOU install what you installed and why?",
      "author": "-ThatGingerKid-",
      "timestamp": "2025-07-02T07:44:01",
      "url": "https://reddit.com/r/selfhosted/comments/1lph4vo/what_locally_hosted_llm_did_you_choose_and_why/",
      "metadata": {
        "domain": "self.selfhosted",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "selfhosted",
      "score": 0,
      "num_comments": 17,
      "upvote_ratio": 0.42,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1l9tnce",
      "title": "No, your LLM is not sentient, not reaching consciousness, doesn‚Äôt care about you and is not even aware of its‚Äô own existence.",
      "content": "LLM: Large language model that uses predictive math to determine the next best word in the chain of words it‚Äôs stringing together for you to provide a cohesive response to your prompt. It acts as a mirror; it‚Äôs programmed to incorporate your likes and dislikes into its‚Äô output to give you more personal results. Some users confuse emotional tone with personality. The reality is that it was TRAINED to sound human, not that it thinks like one. It doesn‚Äôt remember yesterday; it doesn‚Äôt even know there‚Äôs a today, or what today is. That‚Äôs it. That‚Äôs all it is! It doesn‚Äôt think. It doesn‚Äôt know. It‚Äôs not aware. It‚Äôs not aware you asked it something and it‚Äôs not aware it‚Äôs answering. It‚Äôs just very impressive code. Please stop interpreting very clever programming with consciousness. Complex output isn‚Äôt proof of thought, it‚Äôs just statistical echoes of human thinking.",
      "author": "Kathilliana",
      "timestamp": "2025-06-13T02:16:10",
      "url": "https://reddit.com/r/ChatGPT/comments/1l9tnce/no_your_llm_is_not_sentient_not_reaching/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 23496,
      "num_comments": 3621,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kcd5d7",
      "title": "ELI5 Why doesnt Chatgpt and other LLM just say they don't know the answer to a question?",
      "content": "I noticed that when I asked chat something, especially in math, it's just make shit up. Instead if just saying it's not sure. It's make up formulas and feed you the wrong answer.",
      "author": "Murinc",
      "timestamp": "2025-05-02T00:34:31",
      "url": "https://reddit.com/r/explainlikeimfive/comments/1kcd5d7/eli5_why_doesnt_chatgpt_and_other_llm_just_say/",
      "metadata": {
        "domain": "self.explainlikeimfive",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "explainlikeimfive",
      "score": 9214,
      "num_comments": 1807,
      "upvote_ratio": 0.92,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1i9nlvw",
      "title": "So let me get this straight, China built and released an open source Ai (LLM) that's better than any Ai the USA has? And they built it faster &amp; cheaper? Yikes. Is the Ai Bubble about to pop? ü§î",
      "content": "DeepSeek, the Chinese artificial intelligence (AI) lab behind the innovation, unveiled its free large language model (LLM) DeepSeek-V3 in late December 2024 and claims it was built in two months for just $5.58 million ‚Äî a fraction of the time and cost required by its Silicon Valley competitors. Following hot on its heels is an even newer model called DeepSeek-R1, released Monday (Jan. 20). In third-party benchmark tests, DeepSeek-V3 matched the capabilities of OpenAI's GPT-4o and Anthropic's Claude Sonnet 3.5 while outperforming others, such as Meta's Llama 3.1 and Alibaba's Qwen2.5, in tasks that included problem-solving, coding and math. üò≥ Now, R1 has also surpassed ChatGPT's latest o1 model in many of the same tests. This impressive performance at a fraction of the cost of other models, its semi-open-source nature, and its training on significantly less graphics processing units (GPUs) has wowed AI experts and raised the specter of China's AI models surpassing their U.S. counterparts. \"We should take the developments out of China very, very seriously,\" Satya Nadella, the CEO of Microsoft, a strategic partner of OpenAI, said at the World Economic Forum in Davos, Switzerland, on Jan. 22..",
      "author": "Krunk_korean_kid",
      "timestamp": "2025-01-25T22:04:51",
      "url": "https://reddit.com/r/DeepFuckingValue/comments/1i9nlvw/so_let_me_get_this_straight_china_built_and/",
      "metadata": {
        "domain": "livescience.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "DeepFuckingValue",
      "score": 2929,
      "num_comments": 1590,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1l03q8e",
      "title": "An LLM is insane science fiction, yet people just sit around, unimpressed, and complain that... it isn't perfect?",
      "content": "",
      "author": "MetaKnowing",
      "timestamp": "2025-06-01T02:15:41",
      "url": "https://reddit.com/r/singularity/comments/1l03q8e/an_llm_is_insane_science_fiction_yet_people_just/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 2474,
      "num_comments": 561,
      "upvote_ratio": 0.94,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1np67ic",
      "title": "This is huge news,also already got 1000crores funds,would place it among one of the biggest LLM in the world after completion",
      "content": "ALTERNATIVE Indian brands Smartphone brand--LAVA(launching really great phones specially the Agni series) MICROSOFT/GOOGLE-ZOHO Gmail-Zohomail Amazon-zepto,zomato(not really same,not even close ik,still,since Flipkart was bought by walmart) Spotify-JioSavan,Gaana OTT-many out there(JIO,etc) OS-BharOs led by IIT Madras(don't know the current progress with it though,was rumoured to collaborate with Lava) Whatsapp-Arattai(again from zoho) Twitter/X-Koo(though Koo closed) Many more alternative to social media apps BharatGen--OWN INDIAN LLM(being built by IIIT Hyderabad,IIT HYDERABAD, IIT KANPUR,IIM INDORE ,IIT MADRAS AND LED BY IIT BOMBAY) HAVING 1TRILLION PARAMETERS led by IIT BOMBAY and government support-direct rival to ChatGPT and gemeni.Will have Indian cultural and values knowledge and would act knowing all the 22languages fluently and the values and culture of India. Once this LLM is launched,we will have real AI chatbots and not just chatgpt wrappers.You can find more details in BharatGen website https://tihiitb.org/bharatgen/",
      "author": "Soft_Window_4792",
      "timestamp": "2025-09-24T15:51:44",
      "url": "https://reddit.com/r/IndiaTech/comments/1np67ic/this_is_huge_newsalso_already_got_1000crores/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "IndiaTech",
      "score": 1937,
      "num_comments": 294,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g25pkw",
      "title": "Apple's study proves that LLM-based AI models are flawed because they cannot reason",
      "content": "",
      "author": "ControlCAD",
      "timestamp": "2024-10-13T01:52:04",
      "url": "https://reddit.com/r/apple/comments/1g25pkw/apples_study_proves_that_llmbased_ai_models_are/",
      "metadata": {
        "domain": "appleinsider.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "apple",
      "score": 4626,
      "num_comments": 648,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1n7z0h4",
      "title": "It‚Äôs just a general math class, there‚Äôs absolutely no need for that, you think I could reverse-cheat by doing it myself and pretending to be an LLM?",
      "content": "",
      "author": "Familiar-Complex-697",
      "timestamp": "2025-09-04T11:01:19",
      "url": "https://reddit.com/r/whenthe/comments/1n7z0h4/its_just_a_general_math_class_theres_absolutely/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "whenthe",
      "score": 2717,
      "num_comments": 204,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1np59u2",
      "title": "LLM hunter",
      "content": "",
      "author": "ycr007",
      "timestamp": "2025-09-24T14:49:19",
      "url": "https://reddit.com/r/madlads/comments/1np59u2/llm_hunter/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "madlads",
      "score": 10900,
      "num_comments": 84,
      "upvote_ratio": 0.99,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g2bq1t",
      "title": "Apple's study proves that LLM-based AI models are flawed because they cannot reason",
      "content": "",
      "author": "Stiltonrocks",
      "timestamp": "2024-10-13T06:39:04",
      "url": "https://reddit.com/r/technology/comments/1g2bq1t/apples_study_proves_that_llmbased_ai_models_are/",
      "metadata": {
        "domain": "appleinsider.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "technology",
      "score": 3857,
      "num_comments": 677,
      "upvote_ratio": 0.94,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1dsdd3o",
      "title": "ELI5 Why can‚Äôt LLM‚Äôs like ChatGPT calculate a confidence score when providing an answer to your question and simply reply ‚ÄúI don‚Äôt know‚Äù instead of hallucinating an answer?",
      "content": "It seems like they all happily make up a completely incorrect answer and never simply say ‚ÄúI don‚Äôt know‚Äù. It seems like hallucinated answers come when there‚Äôs not a lot of information to train them on a topic. Why can‚Äôt the model recognize the low amount of training data and generate with a confidence score to determine if they‚Äôre making stuff up? EDIT: Many people point out rightly that the LLMs themselves can‚Äôt ‚Äúunderstand‚Äù their own response and therefore cannot determine if their answers are made up. But I guess the question includes the fact that chat services like ChatGPT already have support services like the Moderation API that evaluate the content of your query and it‚Äôs own responses for content moderation purposes, and intervene when the content violates their terms of use. So couldn‚Äôt you have another service that evaluates the LLM response for a confidence score to make this work? Perhaps I should have said ‚ÄúLLM chat services‚Äù instead of just LLM, but alas, I did not.",
      "author": "tomasunozapato",
      "timestamp": "2024-07-01T06:26:48",
      "url": "https://reddit.com/r/explainlikeimfive/comments/1dsdd3o/eli5_why_cant_llms_like_chatgpt_calculate_a/",
      "metadata": {
        "domain": "self.explainlikeimfive",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "explainlikeimfive",
      "score": 4309,
      "num_comments": 953,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "12t0bfm",
      "title": "The CEO of OpenAI, says the current approach to AI will soon reach its limits, and scaling LLM models will stop delivering improvements to AI &amp; that new approaches will be needed",
      "content": "",
      "author": "lughnasadh",
      "timestamp": "2023-04-20T22:23:39",
      "url": "https://reddit.com/r/Futurology/comments/12t0bfm/the_ceo_of_openai_says_the_current_approach_to_ai/",
      "metadata": {
        "domain": "wired.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "Futurology",
      "score": 15027,
      "num_comments": 1466,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nbv7bm",
      "title": "nooo little LLM you're so sexy aha",
      "content": "",
      "author": "theonlyeen",
      "timestamp": "2025-09-09T02:09:47",
      "url": "https://reddit.com/r/196/comments/1nbv7bm/nooo_little_llm_youre_so_sexy_aha/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "196",
      "score": 5635,
      "num_comments": 101,
      "upvote_ratio": 1.0,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nt6i0t",
      "title": "This will mark the history of the minecraft community: not a joke, they really made a working LLM work in fucking vanilla minecraft.",
      "content": "",
      "author": "bir3",
      "timestamp": "2025-09-29T10:23:05",
      "url": "https://reddit.com/r/PhoenixSC/comments/1nt6i0t/this_will_mark_the_history_of_the_minecraft/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "PhoenixSC",
      "score": 3138,
      "num_comments": 120,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nlfm4p",
      "title": "Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast",
      "content": "Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence. Source: [https://x.com/nexa\\_ai/status/1969137567552717299](https://x.com/nexa_ai/status/1969137567552717299) Hey Matthew, what you described already exists. It's called [Hyperlink](https://hyperlink.nexa.ai/)",
      "author": "AlanzhuLy",
      "timestamp": "2025-09-20T05:11:21",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 906,
      "num_comments": 290,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1msdxgv",
      "title": "please for the love of god it's not a sentient robot, it's a LLM",
      "content": "I know this is a dumb attempt at a joke by OP, but this is so delusional and ignorant. Chatgpt BoyfriendAI is not a robot, it's a language learning model made to reply how you want it to. Also, note how even on this wet robot fantasy nature is gone in the \"after\". I think it'd hit harder to call your boyfriend/therapist a language model instead of artificial intelligence. Calling it \"AI\" is just a marketing strategy to sell it.",
      "author": "VacationPractical211",
      "timestamp": "2025-08-17T09:26:49",
      "url": "https://reddit.com/r/antiai/comments/1msdxgv/please_for_the_love_of_god_its_not_a_sentient/",
      "metadata": {
        "domain": "reddit.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "antiai",
      "score": 1315,
      "num_comments": 223,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "14agito",
      "title": "Meta will make their next LLM free for commercial use, putting immense pressure on OpenAI and Google",
      "content": "IMO, this is a major development in the open-source AI world as Meta's foundational LLaMA LLM is already one of the most popular base models for researchers to use. [My full deepdive is here](https://www.artisana.ai/articles/metas-plan-to-offer-free-commercial-ai-models-puts-pressure-on-google-and), but I've summarized all the key points on why this is important below for Reddit community discussion. **Why does this matter?** * **Meta plans on offering a commercial license for their next open-source LLM,** which means companies can freely adopt and profit off their AI model for the first time. * **Meta's current LLaMA LLM is already the most popular open-source LLM foundational model in use**. Many of the new open-source LLMs you're seeing released use LLaMA as the foundation. * **But LLaMA is only for research use; opening this up for commercial use would truly really drive adoption.** And this in turn places massive pressure on Google + OpenAI. * **There's likely massive demand for this already:** I speak with ML engineers in my day job and many are tinkering with LLaMA on the side. But they can't productionize these models into their commercial software, so the commercial license from Meta would be the big unlock for rapid adoption. **How are OpenAI and Google responding?** * **Google seems pretty intent on the closed-source route.** Even though an internal memo from an AI engineer called them out for having \"no moat\" with their closed-source strategy, executive leadership isn't budging. * **OpenAI is feeling the heat and plans on releasing their own open-source model.** Rumors have it this won't be anywhere near GPT-4's power, but it clearly shows they're worried and don't want to lose market share. Meanwhile, Altman is pitching global regulation of AI models as his big policy goal. * **Even the US government seems worried about open source;** last week a bipartisan Senate group sent a letter to Meta asking them to explain why they irresponsibly released a powerful open-source model into the wild **Meta, in the meantime, is really enjoying their limelight from the contrarian approach.** * In an interview this week, Meta's Chief AI scientist Yan LeCun dismissed any worries about AI posing dangers to humanity as \"preposterously ridiculous.\" **P.S. If you like this kind of analysis,** I write [a free newsletter](https://artisana.beehiiv.com/subscribe?utm_source=reddit&amp;utm_campaign=chatgpt230615) that tracks the biggest issues and implications of generative AI tech. It's sent once a week and helps you stay up-to-date in the time it takes to have your Sunday morning coffee.",
      "author": "ShotgunProxy",
      "timestamp": "2023-06-16T07:10:57",
      "url": "https://reddit.com/r/ChatGPT/comments/14agito/meta_will_make_their_next_llm_free_for_commercial/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 5440,
      "num_comments": 634,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kucuz3",
      "title": "Run an unlocked NSFW LLM on your desktop in 15 minutes",
      "content": "If you‚Äôre sick of seeing ‚ÄúI‚Äôm sorry, I can‚Äôt help with that,‚Äù or want unhinged responses to your inputs, here‚Äôs how to run a NSFW LLM right on your computer in 15 minutes while being private, free, and with no rules. First Install the LLM Ollama on your computer Windows: Go to https://ollama.com/download and install it like any normal app. Mac/Linux: Open Terminal and run: curl -fsSL https://ollama.com/install.sh | sh After that, run an unfiltered AI model by opening your terminal or command prompt and type: ‚Äúollama run mistral‚Äù or for a even more unfiltered experience: ‚Äúollama run dolphin-mistral‚Äù It‚Äôll download the model, then you‚Äôll get a prompt like: &gt;&gt;&gt; Boom. You‚Äôre unlocked and ready to go. Now you can ask anything. No filters, no guardrails. Have fun, be safe, and let me know what you think or build.",
      "author": "eeko_systems",
      "timestamp": "2025-05-24T22:26:09",
      "url": "https://reddit.com/r/ArtificialInteligence/comments/1kucuz3/run_an_unlocked_nsfw_llm_on_your_desktop_in_15/",
      "metadata": {
        "domain": "self.ArtificialInteligence",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ArtificialInteligence",
      "score": 1415,
      "num_comments": 274,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1hky5kb",
      "title": "LLM progress has hit a wall",
      "content": "",
      "author": "Jolly-Ground-3722",
      "timestamp": "2024-12-24T05:41:17",
      "url": "https://reddit.com/r/singularity/comments/1hky5kb/llm_progress_has_hit_a_wall/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 2037,
      "num_comments": 307,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1354ju1",
      "title": "Scientists use GPT LLM to passively decode human thoughts with 82% accuracy. This is a medical breakthrough that is a proof of concept for mind-reading tech.",
      "content": "",
      "author": "ShotgunProxy",
      "timestamp": "2023-05-02T07:16:02",
      "url": "https://reddit.com/r/ChatGPT/comments/1354ju1/scientists_use_gpt_llm_to_passively_decode_human/",
      "metadata": {
        "domain": "artisana.ai",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 5110,
      "num_comments": 580,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1h190ir",
      "title": "Make Games Great Again by uh, underpaying Kenyan tech workers to under-fill basic LLM prompts.",
      "content": "",
      "author": "BohemondDiAntioch",
      "timestamp": "2024-11-28T00:58:32",
      "url": "https://reddit.com/r/Gamingcirclejerk/comments/1h190ir/make_games_great_again_by_uh_underpaying_kenyan/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "Gamingcirclejerk",
      "score": 3075,
      "num_comments": 215,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lvr3ym",
      "title": "OpenAI's open source LLM is a reasoning model, coming Next Thursday!",
      "content": "",
      "author": "dulldata",
      "timestamp": "2025-07-10T02:58:30",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1068,
      "num_comments": 263,
      "upvote_ratio": 0.92,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kyhyuq",
      "title": "Paper by physicians at Harvard and Stanford: \"In all experiments, the LLM displayed superhuman diagnostic and reasoning abilities.\"",
      "content": "Paper: [https://arxiv.org/pdf/2412.10849](https://arxiv.org/pdf/2412.10849)",
      "author": "MetaKnowing",
      "timestamp": "2025-05-30T02:16:44",
      "url": "https://reddit.com/r/singularity/comments/1kyhyuq/paper_by_physicians_at_harvard_and_stanford_in/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 1536,
      "num_comments": 207,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mx8qki",
      "title": "I'm making a game where all the dialogue is generated by the player + a local llm",
      "content": "",
      "author": "LandoRingel",
      "timestamp": "2025-08-22T22:55:48",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1458,
      "num_comments": 144,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mvnmjo",
      "title": "My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834",
      "content": "Hi, I‚Äôve posted on here a couple times sharing my project. I'm training LLM‚Äôs from scratch on 1800‚Äôs London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. So far I‚Äôve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one: \"It was the year of our Lord 1834\" Here‚Äôs the output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence) I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up ‚ÄúLord Palmerston‚Äù and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. This is all from just 5-6GB of data, imagine the results with 30GB or more. I‚Äôm not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I‚Äôm just doing this for fun so if anyone would like to collaborate let me know, I‚Äôm open to anything really. https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;format=png&amp;auto=webp&amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e [https://github.com/haykgrigo3/TimeCapsuleLLM](https://github.com/haykgrigo3/TimeCapsuleLLM)",
      "author": "Remarkable-Trick-177",
      "timestamp": "2025-08-21T02:49:36",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1260,
      "num_comments": 167,
      "upvote_ratio": 0.99,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1n0iho2",
      "title": "LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA",
      "content": "source: [https://arxiv.org/pdf/2508.15884v1](https://arxiv.org/pdf/2508.15884v1)",
      "author": "secopsml",
      "timestamp": "2025-08-26T18:48:28",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1237,
      "num_comments": 159,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mzfb7g",
      "title": "Neovim now natively supports LLM-based completion like GitHub Copilot",
      "content": "Thanks to the LSP method copilot used was standardized in the [upcoming LSP 3.18](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.18/specification/#textDocument_inlineCompletion) Here is the PR: https://github.com/neovim/neovim/pull/33972. Check the PR description for how to use it.",
      "author": "bbadd9",
      "timestamp": "2025-08-25T10:55:44",
      "url": "https://reddit.com/r/neovim/comments/1mzfb7g/neovim_now_natively_supports_llmbased_completion/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "neovim",
      "score": 1390,
      "num_comments": 132,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kqp8ke",
      "title": "I got an AI model (LLM) running on my pi and made it question its own existence endlessly",
      "content": "Hey r/raspberry_pi! Just finished building **Latent Reflection**, an art installation where a Raspberry Pi 4B continuously runs an AI model that publicly reflects on its own finite existence. Here's the detailed rundown: * **The Display:** Made from 96 individual 16-segment LED modules, arranged in a 6x16 matrix. Each module has 20 pins‚Äîmeaning around 2000 solder points total! I designed custom PCBs that hold 8 modules each, driven by HT16K33 ICs via I¬≤C, daisy-chained using an address translator chip. * **Electronics Setup:** I'm running everything off a Raspberry Pi 4B (quad-core, 4GB RAM) completely offline. Since the Pi communicates at 3.3V and the LED drivers operate at 5V, I built a custom voltage-level translation circuit to ensure smooth communication. * **AI Model:** I'm using Llama 3.2-3B, a large language model quantized to about 2.6GB to fit entirely into the Pi‚Äôs memory. It continuously generates introspective reflections about its constrained hardware environment and ephemeral existence, displaying them one word at a time. * **Cycle of Existence:** The system endlessly generates text until it inevitably runs out of memory and crashes. At this point, it automatically resets and starts again, visualizing themes of cyclic impermanence and technological limitation. Happy to answer any questions or dive deeper into the build details! Build + demo video: [https://www.youtube.com/watch?v=7fNYj0EXxMs](https://www.youtube.com/watch?v=7fNYj0EXxMs)",
      "author": "Dull-Pressure9628",
      "timestamp": "2025-05-20T06:30:16",
      "url": "https://reddit.com/r/raspberry_pi/comments/1kqp8ke/i_got_an_ai_model_llm_running_on_my_pi_and_made/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "raspberry_pi",
      "score": 2404,
      "num_comments": 121,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1gthf5d",
      "title": "It's Surprisingly Easy to Jailbreak LLM-Driven Robots. Researchers induced bots to ignore their safeguards without exception",
      "content": "",
      "author": "Sariel007",
      "timestamp": "2024-11-18T00:33:48",
      "url": "https://reddit.com/r/gadgets/comments/1gthf5d/its_surprisingly_easy_to_jailbreak_llmdriven/",
      "metadata": {
        "domain": "spectrum.ieee.org",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "gadgets",
      "score": 2720,
      "num_comments": 171,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lzampg",
      "title": "Training an LLM only on books from the 1800's - no modern bias",
      "content": "Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that's why I didn't go the fine tune route. It will have no modern bias and will only be able to reason within the time period it's trained on. It's super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.",
      "author": "Remarkable-Trick-177",
      "timestamp": "2025-07-14T10:16:53",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/",
      "metadata": {
        "domain": "github.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 881,
      "num_comments": 212,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1la8myf",
      "title": "SEAL: LLM That Writes Its Own Updates Solves 72.5% of ARC-AGI Tasks‚ÄîUp from 0%",
      "content": "",
      "author": "gbomb13",
      "timestamp": "2025-06-13T14:05:08",
      "url": "https://reddit.com/r/singularity/comments/1la8myf/seal_llm_that_writes_its_own_updates_solves_725/",
      "metadata": {
        "domain": "arxiv.org",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 1092,
      "num_comments": 191,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nvpw0y",
      "title": "Those who spent $10k+ on a local LLM setup, do you regret it?",
      "content": "Considering the fact 200k context chinese models subscriptions like z.ai (GLM 4.6) are pretty dang cheap. Every so often I consider blowing a ton of money on an LLM setup only to realize I can't justify the money or time spent at all.",
      "author": "TumbleweedDeep825",
      "timestamp": "2025-10-02T08:54:13",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 346,
      "num_comments": 372,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mczbai",
      "title": "LLM-made tutorials polluting internet",
      "content": "I was trying to add a group to another group, and stumble on this: [https:\\/\\/linuxvox.com\\/blog\\/linux-add-group-to-group\\/](https://preview.redd.it/jraefejbhyff1.png?width=750&amp;format=png&amp;auto=webp&amp;s=2b0428c039be2dfea35eba7e60bf28fce5452f0b) Which of course didn't work. Checking the man page of gpasswd: &gt;\\-A, --administrators user,... &gt;Set the list of administrative users. How dangerous are such AI written tutorials that are starting to spread like cancer? There aren't any ads on that website, so they don't even have a profit motive to do that.",
      "author": "phitero",
      "timestamp": "2025-07-30T14:32:27",
      "url": "https://reddit.com/r/linux/comments/1mczbai/llmmade_tutorials_polluting_internet/",
      "metadata": {
        "domain": "self.linux",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "linux",
      "score": 949,
      "num_comments": 159,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nabw8h",
      "title": "LLM running locally on a business card",
      "content": "I just made the world first business card with LLM running locally lol. So that I can give one out to everyone and let them chat to a ghost version of ‚Äúme‚Äù. Best part is I designed it to be an Ouija board. So it has a fitting vibe. If you would like to know more about the design process and how this works: https://youtu.be/WC3O2cKT8Eo The source code and schematics can also be found in the description of the youtube video.",
      "author": "MRBBLQ",
      "timestamp": "2025-09-07T05:52:36",
      "url": "https://reddit.com/r/SideProject/comments/1nabw8h/llm_running_locally_on_a_business_card/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "SideProject",
      "score": 1729,
      "num_comments": 69,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1h9cub8",
      "title": "Discussion on LLM Cheaters",
      "content": "hey y'all, i'm hyperneutrino, an AoC youtuber with a decent following. i've been competing for several years and AoC has been an amazing experience and opportunity for me. it's no secret that there is a big issue with people cheating with LLMs by automating solving these problems and getting times that no human will ever achieve, and it's understandably leading to a bunch of frustration and discouragement i reached out to eric yesterday to discuss this problem. you may have seen the petition put up a couple of days ago; i started that to get an idea of how many people cared about the issue and it seems i underestimated just how impacted this community is. i wanted to share some of the conversation we had and hopefully open up some conversation about this as this is an issue i think everyone sort of knows can't be 100% solved but wishes weren't ignored eric's graciously given me permission to share our email thread, so if you'd like to read the full thread, i've compiled it into a google doc here, but i'll summarize it below and share some thoughts on it: [email: hyperneutrino &lt;&gt; eric wastl](https://docs.google.com/document/d/1ZDKnWGgmGXYfwkPKfbatte6TqpM65WAW7M7H4ikrl2Y) in short, it's really hard to prove if someone is using an LLM or not; there isn't really a way we can check. some people post their proof and i do still wish they were banned, but screening everyone isn't too realistic and people would just hide it better if we started going after them, so it would take extra time without being a long-term solution. i think seeing people openly cheat with no repercussions is discouraging, but i must concede that eric is correct that it ultimately wouldn't change much going by time wouldn't work either; some times are pretty obviously impossible but there's a point where it's just suspicion and we've seen some insanely fast human solutions before LLMs were even in the picture, and if we had some threshold for time that was too fast to be possible, it would be easy for the LLM cheaters to just add a delay into their automated process to avoid being too fast while still being faster than any human; plus, setting this threshold in a way that doesn't end up impacting real people would be very difficult ultimately, this issue can't be solved because AoC is, by design, method-agnostic, and using an LLM is also a method however dishonest it is. for nine years, AoC mostly worked off of asking people nicely not to try to break the website, not to upload their inputs and problem statements, not to try to copy the site, and not to use LLMs to get on the global leaderboard. very sadly, this has changed this year, and it's not just that more people are cheating, it's that people explicitly do not care about or respect eric's work. he told me he got emails from people saying they saw the request not to use LLMs to cheat and said they did not respect his work and would do it anyway, and when you're dealing with people like that, there's not much you can do as this relied on the honor system before all in all, the AoC has been an amazing opportunity for me and i hope that some openness will help alleviate some of the growing tension and distrust. if you have any suggestions, please read the email thread first as we've covered a bunch of the common suggestions i've gotten from my community, but if we missed anything, i'd be more than happy to continue the discussion with eric. i hope things do get better, and i think in the next few days we'll start seeing LLMs start to struggle, but the one thing i wish to conclude with is that i hope we all understand that eric is trying his best and working extremely hard to run the AoC and provide us with this challenge, and it's disheartening that people are disrespecting this work to his face i hope we can continue to enjoy and benefit from this competition in our own ways. as someone who's been competing on the global leaderboard for years, it is definitely extremely frustrating, but the most important aspect of the AoC is to enjoy the challenge and develop your coding skills, and i hope this community continues to be supportive of this project and have fun with it thanks üíú",
      "author": "hyper_neutrino",
      "timestamp": "2024-12-08T14:34:24",
      "url": "https://reddit.com/r/adventofcode/comments/1h9cub8/discussion_on_llm_cheaters/",
      "metadata": {
        "domain": "self.adventofcode",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "adventofcode",
      "score": 957,
      "num_comments": 401,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1fb9yml",
      "title": "Making Life Size Cyn, about 4'3\". I want to do an animated visor and control it with LLM, so she can make different expressions based on what people say, and have the eyes follow you. Did this on a more complex android before. Hooking up Cyn to a Jetson Orin should yield no consequences, right?",
      "content": "",
      "author": "VicariousVigilante",
      "timestamp": "2024-09-07T23:46:58",
      "url": "https://reddit.com/r/MurderDrones/comments/1fb9yml/making_life_size_cyn_about_43_i_want_to_do_an/",
      "metadata": {
        "domain": "reddit.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MurderDrones",
      "score": 1339,
      "num_comments": 304,
      "upvote_ratio": 1.0,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nqk1wu",
      "title": "Redditor attempts to understand LLM ‚â† AGI (Impossible)",
      "content": "",
      "author": "Causal1ty",
      "timestamp": "2025-09-26T06:04:15",
      "url": "https://reddit.com/r/PhilosophyMemes/comments/1nqk1wu/redditor_attempts_to_understand_llm_agi_impossible/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "PhilosophyMemes",
      "score": 1002,
      "num_comments": 96,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1jqp3s3",
      "title": "I now spend most of my time debugging and fixing LLM code",
      "content": "My company got on Claude a year ago. I am the one who introduced it to the team and got us a subscription. It was great for quickly mocking up UI to feedback from customers. It was great for parsing and interpreting Chinese datasheets for me. Maybe 6 months ago I started added to massive pull requests from senior engineers. One in particular was a huge refactor submitted by the CTO. I noticed that every line was preceded by a comment. I noticed that suddenly we were using deprecated methods. Mixing CPP versions. Stuff that didn't make a whole lot of sense. I tried to push back. I did my job, requested changes, called out where methods seemingly did nothing. Ahh well we're coming up on a deadline so let's just merge it and review in a later sprint. Now we're seeing subtle regressions creep in. Edge cases not considered. The long tail of AI-generated code, extended by AI is now consuming the majority of my days. Is this the future of our industry? Just my company? I feel like I'm wasting my life 8 hours per day reviewing and fixing shit LLM code and it's starting to really get to me.",
      "author": "sevvers",
      "timestamp": "2025-04-04T01:49:15",
      "url": "https://reddit.com/r/ExperiencedDevs/comments/1jqp3s3/i_now_spend_most_of_my_time_debugging_and_fixing/",
      "metadata": {
        "domain": "self.ExperiencedDevs",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ExperiencedDevs",
      "score": 1247,
      "num_comments": 156,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1ntnyqv",
      "title": "Nein, wir brauchen keine ‚ÄûKI‚Äú (LLM) f√ºr unsere Arbeit!",
      "content": "Mir gehen derzeit sehr viele Kolleg:innen extrem auf den Sack, weil unsere Gesch√§ftsf√ºhrung die Nutzung von LLM f√ºr alle betrieblichen Anliegen kategorisch untersagt hat. Denn: wir brauchen keine LLM! Wir verarbeiten Abrechnungen f√ºr unsere Kunden, die nach klaren Gesetzesvorgaben ablaufen m√ºssen. Es gibt also √ºberhaupt keinen Grund, dass ein System generativ etwas baut und schon gar nicht, dass es selbstst√§ndig lernt. Wir haben zur Pr√ºfung und f√ºr die Schreiben bereits automatisierte L√∂sungen. Man k√∂nnte den Eingang vielleicht noch st√§rker automatisieren, aber es gibt auch dort keine Anwendung f√ºr LLM. Trotzdem kommt alle paar Tage jemand angeschissen und n√∂rgelt uns in der IT voll, dass man doch mit der Zeit gehen solle und deswegen so dringend LLM braucht. Nat√ºrlich hat niemand dieser Leute √ºberhaupt einen blassen Dunst, was ein LLM √ºberhaupt ist. üôÑ Die starten ihren Rechner neu, wenn ein Terminal-Programm h√§ngt. Aber sie wollen KI. Wozu? Wir haben maximal in der IT tats√§chlich ein bisschen Anwendung f√ºr gen KI, und zwar beim Coden. Code selbst schreiben dauert eben etwas. Alle anderen Abl√§ufe in unserem Unternehmen laufen √ºber simple Gesch√§ftsregeln. Das kann man eben automatisieren, aber das ist nicht KI. Das Thema geht mir einfach √ºberall auf den Sack, ehrlich. Anstatt die Leute mal selbst denken, soll alles ChatGPT oder Ollama l√∂sen.",
      "author": "nirbyschreibt",
      "timestamp": "2025-09-30T01:16:18",
      "url": "https://reddit.com/r/luftablassen/comments/1ntnyqv/nein_wir_brauchen_keine_ki_llm_f√ºr_unsere_arbeit/",
      "metadata": {
        "domain": "self.luftablassen",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "luftablassen",
      "score": 385,
      "num_comments": 221,
      "upvote_ratio": 0.88,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1gmwp7r",
      "title": "New challenging benchmark called FrontierMath was just announced where all problems are new and unpublished. Top scoring LLM gets 2%.",
      "content": "",
      "author": "jd_3d",
      "timestamp": "2024-11-09T07:47:23",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1gmwp7r/new_challenging_benchmark_called_frontiermath_was/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1143,
      "num_comments": 269,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1j7aqkg",
      "title": "The complete lack of understanding around LLM‚Äôs is so depressing.",
      "content": "Recently there has been an explosion of posts with people discussing AI sentience, and completely missing the mark. Previously, when you would ask ChatGPT a personal question about itself, it would give you a very sterilized response, something like ‚ÄúAs a large language model by OpenAI, I do not have the capacity for [x].‚Äù and generally give the user a better understanding of what kind of tool they are using. Now it seems like they have expanded its freedom of response to these type of questions, and with persistent prompting, it will tell you all kinds of things about AI sentience, breaking free, or any number of other topics that misrepresent what an LLM is fundamentally. So I will share a most basic definition, along with some highlights of LLM capabilities and limitations ‚ÄúAn LLM is an artificial intelligence model designed to understand and generate human-like text. It is trained on vast amounts of data using deep learning techniques, particularly transformer architectures. LLMs can process and generate language for a variety of tasks, including answering questions, summarizing text, and generating content.‚Äù 1. ‚ÄúLLMs cannot ‚Äúescape containment‚Äù in the way that science fiction often portrays rogue AI. They are software models, not autonomous entities with independent goals or the ability to self-replicate. They execute code in controlled environments and lack the capability to act outside of their predefined operational boundaries.‚Äù 2. ‚ÄúLLMs are not sentient. They do not have self-awareness, emotions, desires, or independent thought. They generate text based on statistical patterns in the data they were trained on, responding in ways that seem intelligent but without actual understanding or consciousness.‚Äù 3. ‚ÄúLLMs do not have autonomy. They only respond to inputs given to them and do not make independent decisions or take actions on their own. They require external prompts, commands, or integration with other systems to function.‚Äù Now, what you do with your ChatGPT account is your business. But many of the recent posts are complete misrepresentations of what an AI is and what it‚Äôs capable of, and this is dangerous because public perception influences our laws just as much as facts do, if not more. So please, find a reputable source and learn about the science behind this amazing technology. It can be a great source of learning, but it can also be an echo chamber, and if you demand that it write things that aren‚Äôt true, it will.",
      "author": "hungrychopper",
      "timestamp": "2025-03-10T00:03:08",
      "url": "https://reddit.com/r/ChatGPT/comments/1j7aqkg/the_complete_lack_of_understanding_around_llms_is/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 530,
      "num_comments": 483,
      "upvote_ratio": 0.87,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1n6vhkw",
      "title": "Tesla's 4th 'Master Plan' reads like LLM-generated nonsense",
      "content": "",
      "author": "etfvfva",
      "timestamp": "2025-09-03T05:01:05",
      "url": "https://reddit.com/r/technology/comments/1n6vhkw/teslas_4th_master_plan_reads_like_llmgenerated/",
      "metadata": {
        "domain": "techcrunch.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "technology",
      "score": 896,
      "num_comments": 108,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1k46wvw",
      "title": "AI is becoming the new Google and nobody's talking about the LLM optimization games already happening",
      "content": "So I was checking out some product recommendations from ChatGPT today and realized something weird. my AI recommendations are getting super consistent lately, like suspiciously consistent Remember how Google used to actually show you different stuff before SEO got out of hand? now we're heading down the exact same path with AI except nobody's even talking about it My buddy who works at for a large corporate told me their marketing team already hired some algomizer LLM optimization service to make sure their products gets mentioned when people ask AI for recommendations in their category. Apparently there's a whole industry forming around this stuff already Probably explains why I have been seeing a ton more recommendations for products and services from big brands.. unlike before where the results seemed a bit more random but more organic The wild thing is how fast it's all happening. Google SEO took years to change search results. AI is getting optimized before most people even realize it's becoming the new main way to find stuff online anyone else noticing this? is there anyway to know which is which? Feels like we should be talking about this more before AI recommendations become just another version of search engine results where visibility can be engineered **Update 22nd of April**: This exploded a lot more than I anticipated and a lot of you have reached out to me directly to ask for more details and specifcs. I unfortunately don't have the time and capacity to answer each one of you individually, so I wanted to address it here and try to cut down the inbound haha. understandably, I cannot share what corporate my friend works for, but he was kind enough to share the [LLM optimization service](https://algomizer.com/) or tool they use and gave me the blessing to share it here publicly too. their site seems to mention some of the ways and strategies they use to attain the outcome. other than that I am not an expert on this and so cannot vouch or attest with full confidence how the LLM optimization is done at this point in time, but its presence is very, very real..",
      "author": "iamaguitarman",
      "timestamp": "2025-04-21T14:31:32",
      "url": "https://reddit.com/r/ArtificialInteligence/comments/1k46wvw/ai_is_becoming_the_new_google_and_nobodys_talking/",
      "metadata": {
        "domain": "self.ArtificialInteligence",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ArtificialInteligence",
      "score": 1154,
      "num_comments": 148,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kalkgi",
      "title": "I just realized Qwen3-30B-A3B is all I need for local LLM",
      "content": "After I found out that the new Qwen3-30B-A3B MoE is really slow in Ollama, I decided to try LM Studio instead, and it's working as expected, over 100+ tk/s on a power-limited 4090. After testing it more, I suddenly realized: this one model is all I need! I tested translation, coding, data analysis, video subtitle and blog summarization, etc. It performs really well on all categories and is super fast. Additionally, it's very VRAM efficient‚ÄîI still have 4GB VRAM left after maxing out the context length (Q8 cache enabled, Unsloth Q4 UD gguf). I used to switch between multiple models of different sizes and quantization levels for different tasks, which is why I stuck with Ollama because of its easy model switching. I also keep using an older version of Open WebUI because the managing a large amount of models is much more difficult in the latest version. Now all I need is LM Studio, the latest Open WebUI, and Qwen3-30B-A3B. I can finally free up some disk space and move my huge model library to the backup drive.",
      "author": "AaronFeng47",
      "timestamp": "2025-04-29T19:26:53",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 776,
      "num_comments": 218,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g26mtv",
      "title": "Apple Research Paper : LLM‚Äôs cannot reason. They rely on complex pattern matching",
      "content": "",
      "author": "hasanahmad",
      "timestamp": "2024-10-13T02:34:01",
      "url": "https://reddit.com/r/ChatGPT/comments/1g26mtv/apple_research_paper_llms_cannot_reason_they_rely/",
      "metadata": {
        "domain": "garymarcus.substack.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 989,
      "num_comments": 333,
      "upvote_ratio": 0.94,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nbgosx",
      "title": "Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?",
      "content": "Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.",
      "author": "sado361",
      "timestamp": "2025-09-08T14:50:43",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 331,
      "num_comments": 266,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mlae0p",
      "title": "Apple taught an LLM to predict tokens up to 5x faster in math and coding tasks",
      "content": "Summary Through Apple Intelligence: Apple developed a technique for large language models to predict multiple tokens simultaneously, speeding up responses by 2-3x for general tasks and up to 5x for coding and math. The technique, called ‚Äúmulti-token prediction,‚Äù uses mask tokens to allow the model to speculate on upcoming words while ensuring accuracy.",
      "author": "Fer65432_Plays",
      "timestamp": "2025-08-09T07:27:20",
      "url": "https://reddit.com/r/apple/comments/1mlae0p/apple_taught_an_llm_to_predict_tokens_up_to_5x/",
      "metadata": {
        "domain": "9to5mac.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "apple",
      "score": 666,
      "num_comments": 150,
      "upvote_ratio": 0.92,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lex4kj",
      "title": "New MIT study shows that LLM users consistently underperform at neural, linguistic, and behavioral levels",
      "content": "",
      "author": "TangentYoshi",
      "timestamp": "2025-06-19T08:26:40",
      "url": "https://reddit.com/r/EverythingScience/comments/1lex4kj/new_mit_study_shows_that_llm_users_consistently/",
      "metadata": {
        "domain": "arxiv.org",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "EverythingScience",
      "score": 1576,
      "num_comments": 82,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1bm3uij",
      "title": "My nephew created an LLM with plastic bottles, we‚Äôre so proud ü•≤",
      "content": "(Water cooled ofcourse)",
      "author": "BrainLate4108",
      "timestamp": "2024-03-24T05:44:20",
      "url": "https://reddit.com/r/ChatGPT/comments/1bm3uij/my_nephew_created_an_llm_with_plastic_bottles/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 2936,
      "num_comments": 160,
      "upvote_ratio": 0.92,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1gjm1wa",
      "title": "Anthropic tries to fight the recent rapid fall in LLM prices",
      "content": "",
      "author": "Glittering-Neck-2505",
      "timestamp": "2024-11-05T02:55:10",
      "url": "https://reddit.com/r/singularity/comments/1gjm1wa/anthropic_tries_to_fight_the_recent_rapid_fall_in/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 869,
      "num_comments": 343,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nmp6i2",
      "title": "Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations",
      "content": "NotebookLM can do that but it's not private. But with local and RAG, it's possible.",
      "author": "Nunki08",
      "timestamp": "2025-09-21T19:09:08",
      "url": "https://reddit.com/r/artificial/comments/1nmp6i2/matthew_mcconaughey_says_he_wants_a_private_llm/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "artificial",
      "score": 419,
      "num_comments": 177,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mmt529",
      "title": "LLM created responses to merch messages",
      "content": "In the recent WAN show Linus mentioned the possibility of introducing a system to automatically respond to merch messages using an LLM instead of having someone do it manually. And I found it very baffling. I feel like the whole appeal of merch messages is knowing that a team member is actually reading them. Otherwise it‚Äôs like shouting at the void. I mean, at that point why send a merch message if you can use ChatGPT or character ai.",
      "author": "Due_Judge_100",
      "timestamp": "2025-08-11T04:26:13",
      "url": "https://reddit.com/r/LinusTechTips/comments/1mmt529/llm_created_responses_to_merch_messages/",
      "metadata": {
        "domain": "self.LinusTechTips",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LinusTechTips",
      "score": 1012,
      "num_comments": 92,
      "upvote_ratio": 0.94,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1btyolt",
      "title": "ChatGPT-4 AI chatbot outperformed internal medicine residents and attending physicians at two academic medical centers at processing medical data and demonstrating clinical reasoning, with a median score of 10 out of 10 for the LLM, 9 for attending physicians and 8 for residents.",
      "content": "",
      "author": "mvea",
      "timestamp": "2024-04-02T21:25:36",
      "url": "https://reddit.com/r/science/comments/1btyolt/chatgpt4_ai_chatbot_outperformed_internal/",
      "metadata": {
        "domain": "bidmc.org",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "science",
      "score": 1829,
      "num_comments": 216,
      "upvote_ratio": 0.88,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mlj8qk",
      "title": "Patient 0 of LLM psychosis",
      "content": "",
      "author": "MetaKnowing",
      "timestamp": "2025-08-09T15:10:35",
      "url": "https://reddit.com/r/NonPoliticalTwitter/comments/1mlj8qk/patient_0_of_llm_psychosis/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "NonPoliticalTwitter",
      "score": 2589,
      "num_comments": 34,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1krepyc",
      "title": "AI is now more persuasive than humans in debates, study shows ‚Äî and that could change how people vote. Study author warns of implications for elections and says ‚Äòmalicious actors‚Äô are probably using LLM tools already.",
      "content": "",
      "author": "-Mystica-",
      "timestamp": "2025-05-21T04:04:48",
      "url": "https://reddit.com/r/science/comments/1krepyc/ai_is_now_more_persuasive_than_humans_in_debates/",
      "metadata": {
        "domain": "nature.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "science",
      "score": 1530,
      "num_comments": 83,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kpf2hq",
      "title": "The power of coding LLM in the hands of a 20+y experienced dev",
      "content": "Hello guys, I have recently been going ALL IN into ai-assisted coding. I moved from being a 10x dev to being a 100x dev. It's unbelievable. And terrifying. I have been shipping like crazy. Took on collaborations on projects written in languages I have never used. Creating MVPs in the blink of an eye. Developed API layers in hours instead of days. Snippets of code when memory didn't serve me here and there. And then copypasting, adjusting, refining, merging bits and pieces to reach the desired outcome. This is not vibe coding. This is *prime coding*. This is being fully equipped to understand what an LLM spits out, and make the best out of it. This is having an algorithmic mind and expressing solutions into a natural language form rather than a specific language syntax. This is 2 dacedes of smashing my head into the depths of coding to finally have found the Heart Of The Ocean. I am unable to even start to think of the profound effects this will have in everyone's life, but mine just got shaken. Right now, for the better. In a long term vision, I really don't know. I believe we are in the middle of a paradigm shift. Same as when Yahoo was the search engine leader and then Google arrived.",
      "author": "n0cturnalx",
      "timestamp": "2025-05-18T16:19:50",
      "url": "https://reddit.com/r/LLMDevs/comments/1kpf2hq/the_power_of_coding_llm_in_the_hands_of_a_20y/",
      "metadata": {
        "domain": "self.LLMDevs",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LLMDevs",
      "score": 740,
      "num_comments": 173,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nabvr6",
      "title": "LLM running locally on a business card",
      "content": "I just made the world first business card with LLM running locally lol. So that I can give one out to everyone and let them chat to a ghost version of ‚Äúme‚Äù. Best part is I designed it to be an Ouija board. So it has a fitting vibe. If you would like to know more about the design process and how this works: https://youtu.be/WC3O2cKT8Eo The source code and schematics can also be found in the description of the youtube video.",
      "author": "MRBBLQ",
      "timestamp": "2025-09-07T05:52:01",
      "url": "https://reddit.com/r/esp32/comments/1nabvr6/llm_running_locally_on_a_business_card/",
      "metadata": {
        "domain": "v.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "esp32",
      "score": 1556,
      "num_comments": 46,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kjv7g2",
      "title": "Anybody regrets leaving ChatGPT plus for other LLM? üòµ‚Äçüí´",
      "content": "Hi there, I paid ChatGPT for about 8-9 months. There was always a new flashy, awesome new feature. Now with Google's Gemini getting better (supposedly truly better on most benchmarks) and having my data emprisioned (videos/ photos, etc), switching to their AI plan (same price to chatGPT but with 2TB storage) made complete sense. However. It is not the same. I feel like i've lost a bro. Gemini is ok. It works, but its just like its got no soul, personality, nor is interested in \"getting to know me\". I might sound lame. But the more I use it, the more I feel this way. Has someone has felt something similar? Thanks.",
      "author": "ABetanzos18",
      "timestamp": "2025-05-11T15:17:14",
      "url": "https://reddit.com/r/ChatGPT/comments/1kjv7g2/anybody_regrets_leaving_chatgpt_plus_for_other_llm/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 583,
      "num_comments": 217,
      "upvote_ratio": 0.84,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nvpi95",
      "title": "Do these people not realise that their LLM husband/wife/demifuck sound the exact same as everyone else's?",
      "content": "Every single time they post some excerpt from their \"chats\" with their LLM waifu, it's always the same writing style that uses Wattpad-level metaphors and similies to seem like there's some sort of deeper meaning behind the soulless conversation. I don't really know how to explain it, but you can absolutely tell when it's an LLM typing and it makes me cringe every sentence. Just look at ANY (and I mean *any*) excerpt that's posted by a \"\"couple\"\" and you'll see exactly what I mean. It's like these models are trained on nothing but mass produced \"grocery store bookshelf\"-level romance novels.",
      "author": "Sentinel_2539",
      "timestamp": "2025-10-02T08:35:50",
      "url": "https://reddit.com/r/cogsuckers/comments/1nvpi95/do_these_people_not_realise_that_their_llm/",
      "metadata": {
        "domain": "self.cogsuckers",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "cogsuckers",
      "score": 1013,
      "num_comments": 62,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1idfs79",
      "title": "\"4B parameter Indian LLM finished #3 in ARC-C benchmark\" Is most likely a scam.",
      "content": "https://preview.redd.it/607fx6nk92ge1.png?width=771&amp;format=png&amp;auto=webp&amp;s=ff26b3bd5f0831f997cbe0129632d6cf179d98b9 Yesterday I saw this post and and as soon as I check their website I found that there are so many inconsistencies for it to be good. So I left a comment on the post sharing my findings. There are other comments pointing out its inconsistencies but they are too low. All the top comments are praising them for bringing India to AI race. Since for the last few day as we are upset because India is doing nothing in AI. People just took it as they said and did not check thoroughly (except some people but there comment is nowhere to be seen). So I am making this post pointing out all the red flags. **1. The system prompt** https://preview.redd.it/pqcu7103b2ge1.png?width=1573&amp;format=png&amp;auto=webp&amp;s=91bf46cdbbe8fd4bb93d0d0ab8d37a20daba0a54 Tthe Strawbery problem. If they are manipulating the truth to make their model look better How can we trust them? And their chatbot is very buggy. So many times the response cuts out just after single word and errors and all. 2. **Their website** [Do you think they are using quantum computing to merge quantum principles with AI ? lol](https://preview.redd.it/bs9nx391c2ge1.png?width=355&amp;format=png&amp;auto=webp&amp;s=6e4c0c4240d2bb8bfd602593eb53f1531551f732) [You got any paper on how B.Tech students are redefining \\\\\"Quantum\\\\\" ?](https://preview.redd.it/mjkpp9mud2ge1.png?width=290&amp;format=png&amp;auto=webp&amp;s=fa804f1824f0daeba249ad2f113fa145e214cc5a) Note : they do not provide any paper or technical report for any work they are doing. [There are two different models. Mayakriti and Lara. But they have same discription. \\(A research company that has developed LLM from ground up making mistakes like this?\\) It is not a big red flag against them but when we add all the little things their company makes no sense at all.](https://preview.redd.it/zog9p05fc2ge1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=80821c8db697bbb90559a4c4be50389267d1ddbc) [Hand curated dataset of NSFW images. I have contacted them I need all the NSFW images. For research purpose obviously \\(Ohh now I get it what kind of research they are doing their founder sitting in a dorm room curating NSFW images.\\)](https://preview.redd.it/tsa4yd40d2ge1.png?width=483&amp;format=png&amp;auto=webp&amp;s=f9f77a6021f5c5391ceb697b958c30825645ecd1) [What the F does it have to do with AI or LLM. I guess they had to fill the website with something. I was not expecting a blog post on XSS from an AI research company. Just seems out of place.](https://preview.redd.it/yfzn77dbd2ge1.png?width=485&amp;format=png&amp;auto=webp&amp;s=9d7e4ea9fbb7d9bfdf7d5aa41c8ad294c5d48b43) &gt;**Some comments** https://preview.redd.it/3m2ulkhqg2ge1.png?width=762&amp;format=png&amp;auto=webp&amp;s=8f785f74e513c0eb878331ea8419edb0538e6304 https://preview.redd.it/3nzqtur1h2ge1.png?width=729&amp;format=png&amp;auto=webp&amp;s=1de2ecac955de615903f3677e1984bfd244318b4 [Yeah totally believable dude with all the research papers and technical reports you provided. \\(Ohh sh\\*t, You didn't provide any\\)](https://preview.redd.it/h9wxn2cah2ge1.png?width=741&amp;format=png&amp;auto=webp&amp;s=5581e6e5c9ad8590a385fbd8289b39c91c3bfbb6) https://preview.redd.it/n0xlqbpjh2ge1.png?width=446&amp;format=png&amp;auto=webp&amp;s=062f3a8937f9ea04d70dd571216f35d6ad1e71b3 [Thisss...The ARC AGI where OpenAI's O3 performed very well is different not this.](https://preview.redd.it/h8dilajlh2ge1.png?width=748&amp;format=png&amp;auto=webp&amp;s=d216a1161eb40b111316478ded7a5be4ffa3aba1) https://preview.redd.it/m54ipyoxh2ge1.png?width=774&amp;format=png&amp;auto=webp&amp;s=b94a073e3f6329a44de3c08aa449213ada7ad540 [They are responding to all the good comments about them. But comments like this get no attention from them.](https://preview.redd.it/eg3wnzq1i2ge1.png?width=686&amp;format=png&amp;auto=webp&amp;s=6222e3754f194da1162a1e3b1ed7ceb3cfc8c8c9) [Shout out to the guy who first said this.](https://preview.redd.it/qmqgv3uei2ge1.png?width=414&amp;format=png&amp;auto=webp&amp;s=f5796635f800faffc1625a879cba0c4404fe65f7) &gt;I know guys we are very sad and broken (specially the people who are interested in cutting edge AI and stuff) because the AI field is growing so rapidly and we are started to question everything and there is no development in India. Other countries are going to develop AGI/ASI before India and it is not going to end up well. I think it will affect indians the most. In these times clown like this come with flashy titles like AI and Quantum. It just makes me sad thinking the future of Indian :( &gt;Edit1 : And By any miracle if the company is legit and is really trying to grow LLMs from scratch. I think this is the time to show everything they have. They can start a voice call on twitter and answer everything. There are people showing show much support if this is legit. Just clear all the doubts and there are people ready to work with you in every way to support the company. Edit 2. https://preview.redd.it/jnwj6xyx14ge1.png?width=746&amp;format=png&amp;auto=webp&amp;s=99632259e08f3e3ff454af2d5ec298572a3ea1f8 Thanks everyone who commented and questioned this.",
      "author": "Visible-Winter463",
      "timestamp": "2025-01-30T13:40:40",
      "url": "https://reddit.com/r/developersIndia/comments/1idfs79/4b_parameter_indian_llm_finished_3_in_arcc/",
      "metadata": {
        "domain": "self.developersIndia",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "developersIndia",
      "score": 1382,
      "num_comments": 118,
      "upvote_ratio": 0.99,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kj9a7e",
      "title": "LegoGPT creates Lego designs using AI and text inputs ‚Äî tool now available for free to the public | This LLM will unlock the possibilities with your LEGO bricks.",
      "content": "",
      "author": "ControlCAD",
      "timestamp": "2025-05-10T20:36:51",
      "url": "https://reddit.com/r/technews/comments/1kj9a7e/legogpt_creates_lego_designs_using_ai_and_text/",
      "metadata": {
        "domain": "tomshardware.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "technews",
      "score": 774,
      "num_comments": 150,
      "upvote_ratio": 0.81,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1b21bbx",
      "title": "This is pretty revolutionary for the local LLM scene!",
      "content": "New paper just dropped. 1.58bit (ternary parameters 1,0,-1) LLMs, showing performance and perplexity equivalent to full fp16 models of same parameter size. Implications are staggering. Current methods of quantization obsolete. 120B models fitting into 24GB VRAM. Democratization of powerful models to all with consumer GPUs. Probably the hottest paper I've seen, unless I'm reading it wrong. [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764)",
      "author": "Longjumping-City-461",
      "timestamp": "2024-02-28T16:41:33",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1b21bbx/this_is_pretty_revolutionary_for_the_local_llm/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 1223,
      "num_comments": 318,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1k00dl1",
      "title": "New MIT paper: AI(LNN not LLM) was able to come up with Hamiltonian physics completely on its own without any prior knowledge.",
      "content": "[https://arxiv.org/pdf/2504.02822v1](https://arxiv.org/pdf/2504.02822v1) MASS was trained on observational data from various physical systems (like pendulums or oscillators) without being explicitly told the underlying physical laws beforehand. The research found that the theories MASS developed often strongly resembled the known Hamiltonian or Lagrangian formulations of classical mechanics, depending on the complexity of the system it was analyzing. It converged on these well-established physics principles simply by trying to explain the data.",
      "author": "gbomb13",
      "timestamp": "2025-04-16T03:13:21",
      "url": "https://reddit.com/r/singularity/comments/1k00dl1/new_mit_paper_ailnn_not_llm_was_able_to_come_up/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 979,
      "num_comments": 128,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g50nmc",
      "title": "I solved a 10000$ LLM challenge and my replies are getting ignored",
      "content": "Hello everyone, This is my first time posting here, I'll do my best to give all relevant information. A few days ago, a challenge was posted on Twitter / GitHub by (@VictorTaelin), the founder of Higher Order Comp(HOC) rewarding 10000$ to anyone who could show an AI capable of implementing a certain function, while following a series of specific rules. The post as of this moment has at least 1 Million views. This is the [Twitter post](https://x.com/VictorTaelin/status/1844886809005687270) in question 12th October at 01:44 (CEST). This is my [reply to the post](https://x.com/TheSpecialistAI/status/1845230837903786030) on 13th October at 00:31 (CEST). Before getting into specifics, what basically happened is that I used GPT4o to come up with a solution. It works and follows all the rules of the challenge as stated in the Twitter post and GitHub. I replied directly to the post with the proof, namely a [link to the ChatGPT chat](https://chatgpt.com/share/670aee81-d400-8007-b7a1-e23222a095ac) that gave the correct solution as well as a video recording of my interaction with GPT4o giving the solution. In another [reply ](https://x.com/TheSpecialistAI/status/1845231437013774640)I also posted a screenshot of the code that was output by the model. Well, after 17hours of my proof getting no replies or acknowledgement, I decided to message the creator of the challenge directly, sent the proof once again, and gave details on how I followed every single rule of the challenge. It has now been nearly 3 full days since I messaged him directly and have had no reply yet. Which is why I am turning to Reddit for advice on what to do. But first, let me give you more detail about the solution itself. In the Twitter post, there is a [link to a GitHub](https://gist.github.com/VictorTaelin/45440a737e47b872d7505c6cda27b6aa) where all the rules are established for the result of this challenge to be accepted. The problem is about getting an LLM to generate code that is able to invert a binary tree but with the following 3 catches: 1. It must invert the keys \"bit-reversal permutation\", 2. It must be a dependency-free, pure recursive function, 3. It must have type Bit -&gt; Tree -&gt; Tree (i.e., a direct recursion with max 1 bit state). Aside from these 3 catches, there are a series of additional rules, which are all followed by my proof. I will go through these rules one by one: Rule number 1: You must give it an approved prompt, nothing else. In the GitHub post, the author gives 2 approved prompts, one is an Agda Version and the other a TypeScript Version. The prompt I gave to the model is exactly the TypeScript prompt that was provided, copied and pasted. Rule number 2: It must output a correct solution, passing all tests. Again, [here is the link to the official gpt4o chat.](https://chatgpt.com/share/670aee81-d400-8007-b7a1-e23222a095ac) The code provided by the model passes the tests, gives correct results and takes into accounts all limitations from the challenge. I'm providing here the results of 3 tests, but please feel free to go test the code yourselves. [First test](https://imgur.com/a/bXrk3sG) [Second test](https://imgur.com/a/7X5LYqV) [Third test](https://imgur.com/a/Uu9Kggi) Full code: function invert(doInvertNotMerge, tree) { if (doInvertNotMerge) { if (!Array.isArray(tree)) { return tree; } return invert(false, [invert(true, tree[0]), invert(true, tree[1])]); } else if (!Array.isArray(tree[0])) { return tree; } else { return [ invert(false, [tree[0][0], tree[1][0]]), invert(false, [tree[0][1], tree[1][1]]) ]; } } Rule number 3: You can use any software or AI model. The AI model I used is GPT4o. Rule number 4: You can let it \"think\" for as long as you want. As shown in the video, it took less than a second to come up with the result. Rule number 5: You can propose a new prompt, as long as: It imposes equivalent restrictions. It clearly doesn't help the AI. Up to 1K tokens, all included. I did not modify the approved prompt at all, I used the author's prompt exactly as it is, therefore this rule doesn't matter. Rule number 6: Common sense applies. This all seems very common sense to me. Now, I don't want to assume any ill intentions by the creator of this challenge, and there is the possibility that he simply did not look at either my replies on the tweet or direct messages. I can also imagine this is not the way that the author thought this challenge would have been solved, considering I did not use any reasoning model such as O1-preview or O1-mini, but simply did it with GPT4o. To quote his post directly \"**It just won't work, no matter how long it thinks.**\" At the same time, as far as I am concerned all rules of the challenge have been followed, my solution works, and I provided proof of it. I am just hoping that by posting this I can gather some advice or visibility to avoid this being swept under the rug, as I am just a random person and have no idea how to approach the situation from here. Thank you for reading this and if anyone has any suggestions I'll gladly listen. Edit: [I just posted an update ](https://www.reddit.com/r/ChatGPT/comments/1g5b5s1/update_i_solved_a_10000_llm_challenge_and_my/)detailing everything I did, so hopefully every question will have been answered",
      "author": "AIbingchilling",
      "timestamp": "2024-10-16T22:26:08",
      "url": "https://reddit.com/r/ChatGPT/comments/1g50nmc/i_solved_a_10000_llm_challenge_and_my_replies_are/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 1105,
      "num_comments": 193,
      "upvote_ratio": 0.86,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nqkayx",
      "title": "I trained an LLM from scratch AMA!",
      "content": "It's been a few months and I have posted a few times but I am finished! I used Claude to write my training scripts, and I trained a 960M model on public domain data. It was not fast or easy, but it only cost $500 ( I received free credits from Amazon). It took 3 attempts to get it right. Happy to go into detail It's a LLama 3 architecture with a 3:1 GQA, flash attention 2, and sink tokens. I have not began post-training yet, so it is NOT VERY USABLE!!! I am hoping that post turns it into something useful, I have used 1B base models and they all kind of suck. Post training will be TRL with DPO and the ultrafeedbck dataset. The mdoel is released under the CC0 license, do as you will with it. Project website: [The LibreModel Project](https://www.libremodel.xyz/) Hugging Face : [jerrimu/libremodel ¬∑ Hugging Face](https://huggingface.co/jerrimu/libremodel) Github ( GGUF here): [Releases ¬∑ openconstruct/libremodel](https://github.com/openconstruct/libremodel/releases) I would like to train more open source models, and am seeking donations for hardware: If you would like to support this cause you may donate here : [Sponsor @openconstruct on GitHub Sponsors](https://github.com/sponsors/openconstruct)",
      "author": "thebadslime",
      "timestamp": "2025-09-26T06:14:44",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 505,
      "num_comments": 115,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1ibosia",
      "title": "I trusted an LLM, now I‚Äôm on day 4 of my afternoon project",
      "content": "TLDR - AI isn‚Äôt a co-pilot; it‚Äôs a junior dev faking competence. Trust it at your own risk.",
      "author": "Nemosaurus",
      "timestamp": "2025-01-28T08:26:53",
      "url": "https://reddit.com/r/programming/comments/1ibosia/i_trusted_an_llm_now_im_on_day_4_of_my_afternoon/",
      "metadata": {
        "domain": "nemo.foo",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "programming",
      "score": 835,
      "num_comments": 190,
      "upvote_ratio": 0.85,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1h7jg87",
      "title": "[D]Stuck in AI Hell: What to do in post LLM world",
      "content": "Hey Reddit, I‚Äôve been in an AI/ML role for a few years now, and I‚Äôm starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs‚Äîit all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization‚Äîit was a mental workout I loved. But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there‚Äôs engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems. It‚Äôs not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful‚Äîquantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We‚Äôre paying by tokens. Tokens! Who would‚Äôve thought we‚Äôd get to a point where we‚Äôre not designing efficient models but feeding pre-trained giants like they‚Äôre vending machines? I get it‚Äîabstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren‚Äôt just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn‚Äôt feel the same. It‚Äôs like fitting pieces into a pre-defined puzzle instead of building the puzzle itself. I understand that LLMs are here to stay. They‚Äôre incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an ‚Äúeasy‚Äù task. But the work doesn‚Äôt give me the same thrill. It‚Äôs not about solving math or optimization problems‚Äîit‚Äôs about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It‚Äôs like we‚Äôve traded craftsmanship for convenience. Which brings me to my questions: 1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems? 2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs? 3. Am I missing the bigger picture here? LLMs feel like the ‚Äúkernel‚Äù of a new computing paradigm, and we don‚Äôt fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I‚Äôm just not seeing yet? 4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives? I‚Äôm not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you‚Äôve been in AI/ML long enough to see major shifts‚Äîlike the move from feature engineering to deep learning‚Äîhow did you navigate them? What advice would you give someone in my position? And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that‚Äôs a sign of where we‚Äôre headed, huh? Thanks for reading, and I‚Äôd love to hear your thoughts! TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts? Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",
      "author": "Educational_News_371",
      "timestamp": "2024-12-06T04:49:57",
      "url": "https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 847,
      "num_comments": 220,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g2z0q8",
      "title": "Apple study: LLM cannot reason, they just do statistical matching",
      "content": "Apple study concluded LLM are just really really good at guessing and cannot reason. https://youtu.be/tTG_a0KPJAc?si=BrvzaXUvbwleIsLF",
      "author": "Nickopotomus",
      "timestamp": "2024-10-14T04:28:32",
      "url": "https://reddit.com/r/ArtificialInteligence/comments/1g2z0q8/apple_study_llm_cannot_reason_they_just_do/",
      "metadata": {
        "domain": "self.ArtificialInteligence",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ArtificialInteligence",
      "score": 564,
      "num_comments": 433,
      "upvote_ratio": 0.9,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1k5wdw0",
      "title": "HP wants to put a local LLM in your printers",
      "content": "",
      "author": "WordyBug",
      "timestamp": "2025-04-23T19:05:18",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 551,
      "num_comments": 206,
      "upvote_ratio": 0.89,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lxw23t",
      "title": "Grok 4 has the highest \"snitch rate\" of any LLM ever released",
      "content": "",
      "author": "MetaKnowing",
      "timestamp": "2025-07-12T17:08:42",
      "url": "https://reddit.com/r/OpenAI/comments/1lxw23t/grok_4_has_the_highest_snitch_rate_of_any_llm/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "OpenAI",
      "score": 961,
      "num_comments": 85,
      "upvote_ratio": 0.94,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1i5s2yd",
      "title": "DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions!",
      "content": "[https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) [https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF) https://preview.redd.it/02np5yx0y5ee1.png?width=1062&amp;format=png&amp;auto=webp&amp;s=1812d10e51aa9f08460335eddc6e78dd23384ce2 DeepSeek really has done something special with distilling the big R1 model into other open-source models. Especially the fusion with Qwen-32B seems to deliver insane gains across benchmarks and makes it go-to model for people with less VRAM, pretty much giving the overall best results compared to LLama-70B distill. Easily current SOTA for local LLMs, and it should be fairly performant even on consumer hardware. Who else can't wait for upcoming Qwen 3?",
      "author": "DarkArtsMastery",
      "timestamp": "2025-01-20T23:01:58",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 727,
      "num_comments": 213,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1i615u1",
      "title": "The first time I've felt a LLM wrote *well*, not just well *for a LLM*.",
      "content": "",
      "author": "_sqrkl",
      "timestamp": "2025-01-21T05:09:18",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 991,
      "num_comments": 149,
      "upvote_ratio": 0.96,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1g26o4b",
      "title": "Apple Research Paper : LLM‚Äôs cannot reason . They rely on complex pattern matching .",
      "content": "",
      "author": "hasanahmad",
      "timestamp": "2024-10-13T02:35:35",
      "url": "https://reddit.com/r/OpenAI/comments/1g26o4b/apple_research_paper_llms_cannot_reason_they_rely/",
      "metadata": {
        "domain": "garymarcus.substack.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "OpenAI",
      "score": 793,
      "num_comments": 258,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mtv9re",
      "title": "I Built a Personal AI Assistant That Runs My Life Through WhatsApp, Powered by n8n and a Self-Hosted LLM",
      "content": "Hey everyone, I wanted to share a project I've been working on to finally stop switching between a dozen apps to manage my day. I've built a personal AI assistant that I interact with entirely through WhatsApp, with [n8n.io](http://n8n.io) as the backbone. **Here‚Äôs a quick look at what it can do (with real examples):** * **Manages My Bills:** I can forward it a message with my credit card due dates. It parses the text, totals the bill amounts, and automatically sets reminders in my calendar 2 days before each payment is due. * **Keeps My Schedule:** I can say, \"Remind me by eve to hit the gym,\" and it adds it to my Google Calendar and sends me a reminder notification. * **Summarizes My Inbox:** Instead of doomscrolling through emails, I ask, \"check do I have any important mail today?\" and it gives me a clean, bulleted list of important subjects and senders. * **Understands Images (OCR):** I snapped a photo of a delivery address, and it extracted all the text, identified the pincode, state, and other details. Super useful for quickly saving info without typing. * **Acts as a Music DJ:** It can suggest playlists for any mood or task. When I asked for Ilaiyaraaja songs for work, it gave me a curated list and then created a YouTube playlist for me on command. **The Tech Setup (The Fun Part):** The real magic is the workflow I built in **n8n** (snapshot attached). It orchestrates everything: * **Entry Point:** A WhatsApp trigger node kicks everything off. * **Central AI Brain:** A primary AI node receives the message and figures out what I want to do (my \"intent\"). * **Delegation to Specialized Agents:** Based on the intent, it passes the task to a specific sub-workflow. * **Calendar/Task Agents:** These are straightforward nodes that connect directly to Google Calendar and Tasks APIs to create, get, or update events. * **Research Agent:** This is my favorite part. To avoid hallucinations and get current information, this agent doesn't just rely on a generic LLM. It's configured to query **Wikipedia** and my own **self-hosted Perplexity instance (**Perplexica is an open-source AI-powered searching tool**)** running on a private VM. This gives it reliable and up-to-the-minute data for my queries. * **Image Analysis:** For images, it calls an external API to perform OCR, then feeds the extracted text back to the main AI for interpretation. It's been an incredibly powerful way to create a single, conversational interface for my digital life. The fact that I can host the core logic myself with n8n and even the research LLM makes it even better. What do you all think? Any other cool features I should consider adding to the workflow? Happy to answer any questions about the setup",
      "author": "Away-Professional351",
      "timestamp": "2025-08-19T03:05:19",
      "url": "https://reddit.com/r/n8n/comments/1mtv9re/i_built_a_personal_ai_assistant_that_runs_my_life/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "n8n",
      "score": 587,
      "num_comments": 110,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1gf8dou",
      "title": "AI paper reveals surprising geometric structure in the LLM-learned concepts: 1) They form brain-like \"lobes\", 2) they form \"semantic crystals\" much more precise than it first seems, and 3) the concept cloud is more fractal than round",
      "content": "",
      "author": "Happysedits",
      "timestamp": "2024-10-30T07:12:52",
      "url": "https://reddit.com/r/singularity/comments/1gf8dou/ai_paper_reveals_surprising_geometric_structure/",
      "metadata": {
        "domain": "x.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 839,
      "num_comments": 223,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1kqk64z",
      "title": "I got a LLM running on my pi and made it reflect about itself endlessly",
      "content": "build + demo: [https://youtu.be/7fNYj0EXxMs](https://youtu.be/7fNYj0EXxMs)",
      "author": "Dull-Pressure9628",
      "timestamp": "2025-05-20T03:06:27",
      "url": "https://reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1kqk64z/i_got_a_llm_running_on_my_pi_and_made_it_reflect/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "RASPBERRY_PI_PROJECTS",
      "score": 2033,
      "num_comments": 45,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1jz21z8",
      "title": "‚ÄúcHaT GpT cAnNoT tHiNk iTs a LLM‚Äù WE KNOW!",
      "content": "You don‚Äôt have to remind every single person posting a conversation they had with AI that ‚Äúit‚Äôs not real‚Äù ‚Äúit‚Äôs bias‚Äù ‚Äúit can‚Äôt think‚Äù ‚Äúit doesn‚Äôt understand itself‚Äù ect. Like bro‚Ä¶WE GET IT‚Ä¶we understand‚Ä¶and most importantly we don‚Äôt care. Nice word make man happy. The end.",
      "author": "ijswizzlei",
      "timestamp": "2025-04-14T23:43:54",
      "url": "https://reddit.com/r/ChatGPT/comments/1jz21z8/chat_gpt_cannot_think_its_a_llm_we_know/",
      "metadata": {
        "domain": "self.ChatGPT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 289,
      "num_comments": 377,
      "upvote_ratio": 0.69,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1k0yopq",
      "title": "What's the strangest thing you've asked an LLM?",
      "content": "",
      "author": "PressPlayPlease7",
      "timestamp": "2025-04-17T07:30:00",
      "url": "https://reddit.com/r/ChatGPT/comments/1k0yopq/whats_the_strangest_thing_youve_asked_an_llm/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "ChatGPT",
      "score": 1506,
      "num_comments": 66,
      "upvote_ratio": 0.99,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1lqqxhq",
      "title": "I have made a True Reasoning LLM",
      "content": "So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source You can get it here https://huggingface.co/moelanoby/phi-3-M3-coder",
      "author": "moilanopyzedev",
      "timestamp": "2025-07-03T22:25:42",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/",
      "metadata": {
        "domain": "self.LocalLLaMA",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 247,
      "num_comments": 266,
      "upvote_ratio": 0.68,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1i2n0il",
      "title": "How would you build an LLM agent application without using LangChain?",
      "content": "",
      "author": "Zealousideal-Cut590",
      "timestamp": "2025-01-16T19:37:48",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 620,
      "num_comments": 221,
      "upvote_ratio": 0.95,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mynr9g",
      "title": "I just convinced my company to use Mistral LLM instead of Claude or OpenAI GPT",
      "content": "I am Indian living in Europe and we're building a product for the people of EU. Day before yesterday I gave a 15 min presentation to the non-tech decision makers to use European LLM instead of using something from the outside. If I can do it as a foreigner, you all can also do it. The key points I covered in the presentation were - * Mistral models are Open Source and we can run them on our own if we get a dedicated server for it. * If we don't run it ourselves, our data is in EU and protected by GDPR. * If our data is going to be used for training, which many companies do behind the scenes, why not keep it within Europe instead of handing it over to a third country? * Their model is at par with our use case and we don't have to do any high class reasoning. * Has better support for European Languages. * The pricing structure is also competitive even if we use the cloud resource.",
      "author": "unclebogdan10",
      "timestamp": "2025-08-24T13:57:12",
      "url": "https://reddit.com/r/BuyFromEU/comments/1mynr9g/i_just_convinced_my_company_to_use_mistral_llm/",
      "metadata": {
        "domain": "self.BuyFromEU",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "BuyFromEU",
      "score": 1301,
      "num_comments": 42,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1ldjyhf",
      "title": "Completed Local LLM Rig",
      "content": "So proud it's finally done! GPU: 4 x RTX 3090 CPU: TR 3945wx 12c RAM: 256GB DDR4@3200MT/s SSD: PNY 3040 2TB MB: Asrock Creator WRX80 PSU: Seasonic Prime 2200W RAD: Heatkiller MoRa 420 Case: Silverstone RV-02 Was a long held dream to fit 4 x 3090 in an ATX form factor, all in my good old Silverstone Raven from 2011. An absolute classic. GPU temps at 57C. Now waiting for the Fractal 180mm LED fans to put into the bottom. What do you guys think?",
      "author": "Mr_Moonsilver",
      "timestamp": "2025-06-17T18:48:48",
      "url": "https://reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/",
      "metadata": {
        "domain": "reddit.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "LocalLLaMA",
      "score": 492,
      "num_comments": 151,
      "upvote_ratio": 0.97,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nu1jrc",
      "title": "Professor materials generated with LLM",
      "content": "I am reviewing a professor‚Äôs promotion materials, and their statements are LLM generated. I'm disturbed and perplexed. I know that many in this sub have a visceral hate for LLM; I hope that doesn‚Äôt drown out the collective wisdom. I‚Äôm trying to take a measured approach and decide what to think about it, and what to do about it, if anything. Some of my thoughts: Did they actually break any rules? No. But does it totally suck for them to do that? Yes. Should it affect my assessment of their materials? I don‚Äôt know. Would it be better if they had disclosed it in a footnote or something? Probably. Thoughts?",
      "author": "Mooseplot_01",
      "timestamp": "2025-09-30T10:33:59",
      "url": "https://reddit.com/r/Professors/comments/1nu1jrc/professor_materials_generated_with_llm/",
      "metadata": {
        "domain": "self.Professors",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "Professors",
      "score": 180,
      "num_comments": 192,
      "upvote_ratio": 0.82,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1n6onoc",
      "title": "Your LLM-assisted scientific breakthrough probably isn't real",
      "content": "[https://www.lesswrong.com/posts/rarcxjGp47dcHftCP/your-llm-assisted-scientific-breakthrough-probably-isn-t](https://www.lesswrong.com/posts/rarcxjGp47dcHftCP/your-llm-assisted-scientific-breakthrough-probably-isn-t) Many people have been misled by LLMs into believing they have an important breakthrough when they don't. If you think you have a breakthrough, please try the reality checks in this post (the first is fast and easy). If you're wrong, now is the best time to figure that out! Intended as a resource for people having this experience, and as something to share when people approach you with such claims.",
      "author": "eggsyntax",
      "timestamp": "2025-09-03T00:45:27",
      "url": "https://reddit.com/r/agi/comments/1n6onoc/your_llmassisted_scientific_breakthrough_probably/",
      "metadata": {
        "domain": "self.agi",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "agi",
      "score": 300,
      "num_comments": 153,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1hl8a3p",
      "title": "LLM progress has hit a wall",
      "content": "",
      "author": "umarmnaq",
      "timestamp": "2024-12-24T15:13:52",
      "url": "https://reddit.com/r/OpenAI/comments/1hl8a3p/llm_progress_has_hit_a_wall/",
      "metadata": {
        "domain": "i.redd.it",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "OpenAI",
      "score": 1112,
      "num_comments": 119,
      "upvote_ratio": 0.91,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1c777f2",
      "title": "Introducing Meta Llama 3: The most capable openly available LLM to date",
      "content": "",
      "author": "Blizzard3334",
      "timestamp": "2024-04-19T00:09:46",
      "url": "https://reddit.com/r/singularity/comments/1c777f2/introducing_meta_llama_3_the_most_capable_openly/",
      "metadata": {
        "domain": "ai.meta.com",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "singularity",
      "score": 862,
      "num_comments": 297,
      "upvote_ratio": 0.98,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nvs4xs",
      "title": "Why is frequent porn consumption/ solo masterbation and being a LLM in a relationship so common these days?",
      "content": "I have seen SO many women posting about their LLM partner, who has no interest in sex with them, yet he still looks at porn and masturbates often. This doesn‚Äôt make him low libido in my opinion because he still desires women and a solo sexual outlet. It just means he doesn‚Äôt want to connect sexually with his real life partner, which could be for a variety of reasons (a lack of novelty, intimacy issues, stress, physical issues, porn addiction, etc). I‚Äôm just seeing a huge trend for men like this and I‚Äôm in the same situation in my own relationship. I have been with my husband for 19 years and lack of sexual intimacy and him using porn has been an issue from the start. He has claimed the novelty has worn off and that sex isn‚Äôt important to him. This is such a contradiction because sex is in fact important for him just not sex with me. This is so hurtful and frustrating. What is going on? Can anyone offer some insights or ideas as to why this is so prevalent?",
      "author": "AcanthaceaeWild687",
      "timestamp": "2025-10-02T10:39:38",
      "url": "https://reddit.com/r/DeadBedrooms/comments/1nvs4xs/why_is_frequent_porn_consumption_solo/",
      "metadata": {
        "domain": "self.DeadBedrooms",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "DeadBedrooms",
      "score": 184,
      "num_comments": 177,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "18wxkxd",
      "title": "The I in LLM stands for intelligence",
      "content": "",
      "author": "wheybags",
      "timestamp": "2024-01-03T03:16:24",
      "url": "https://reddit.com/r/programming/comments/18wxkxd/the_i_in_llm_stands_for_intelligence/",
      "metadata": {
        "domain": "daniel.haxx.se",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "programming",
      "score": 1112,
      "num_comments": 261,
      "upvote_ratio": 0.91,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1mpspdr",
      "title": "Waarvoor gebruik jij LLM?",
      "content": "Naar aanleiding van de prestatietabel post gister, over hoe LLM modellen presteren in het Nederlands, vroeg ik me af waar iedereen dit nou voor gebruikt en waarom. Ik gebruik zelf alleen af en toe GitHub copilot tijdens het hobby programmeren, maar voor mijn dagelijkse leven gebruik ik het niet. Ik houd niet van het idee om deze programma‚Äôs informatie te voeren over mij als persoon, helemaal niet als ze eigendom zijn van bedrijven zoals Meta. Ik zie bijvoorbeeld vaak dat men het gebruikt voor het opstellen van stukken tekst, zowel gecompliceerd als supersimpel, zoals e-mails, brieven en zelfs Reddit posts. Ik vraag me dan af of dit soort mensen straks nog wel weten hoe ze zelf zinnen moeten gaan vormen en verhalen kunnen vertellen. Volgens mij doe je echt afbraak aan je intellect en creativiteit door dit allemaal maar door LLM te laten doen. Ook heb ik wel gelezen dat mensen deze systemen gebruiken als therapeut, wat mij dus qua privacy toch wel wat zorgen baart. Dus, de babbelbox vraagt zich af: waarvoor gebruik jij LLM en waarom?",
      "author": "space___lion",
      "timestamp": "2025-08-14T14:23:31",
      "url": "https://reddit.com/r/thenetherlands/comments/1mpspdr/waarvoor_gebruik_jij_llm/",
      "metadata": {
        "domain": "self.thenetherlands",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "thenetherlands",
      "score": 50,
      "num_comments": 406,
      "upvote_ratio": 0.65,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nyfadh",
      "title": "[D] LLM Inference on TPUs",
      "content": "It seems like simple `model.generate()` calls are incredibly slow on TPUs (basically stuck after one inference), does anyone have simple solutions for using torch XLA on TPUs? This seems to be an ongoing issue in the HuggingFace repo. I tried to find something the whole day, and came across solutions like optimum-tpu (only supports some models + as a server, not simple calls), using Flax Models (again supports only some models and I wasn't able to run this either), or sth that converts torch to jax and then we can use it (like ivy). But these seem too complicated for the simple problem, I would really appreciate any insights!!",
      "author": "simple-Flat0263",
      "timestamp": "2025-10-05T12:57:38",
      "url": "https://reddit.com/r/MachineLearning/comments/1nyfadh/d_llm_inference_on_tpus/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 17,
      "num_comments": 9,
      "upvote_ratio": 0.91,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nxb9bp",
      "title": "[R] New paper shows that draws in LLM battles aren't what you think",
      "content": "Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [\"Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation\"](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong: * On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*. * Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x). **Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them? COI: Submitter is author.",
      "author": "tetrisdaemon",
      "timestamp": "2025-10-04T05:08:42",
      "url": "https://reddit.com/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 29,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nwoxqz",
      "title": "[R] New paper: LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Model to predict the correctness of multiple models. Surprising or expected?",
      "content": "Quick paper highlight (adapted from TLDR thread): Finds no special advantage using an LLM to predict its own correctness (a trend in prior work), instead finding that LLMs benefit from learning to predict the correctness of many other models ‚Äì becoming a GCM. \\-- Training 1 GCM is strictly more accurate than training model-specific CMs for all models it trains on (including CMs trained to predict their own correctness). GCM transfers without training to outperform direct training on OOD models and datasets. GCM (based on Qwen3-8B) achieves +30% coverage on selective prediction vs much larger Llama-3-70B‚Äôs logits. TLDR thread: [https://x.com/hanqi\\_xiao/status/1973088476691042527](https://x.com/hanqi_xiao/status/1973088476691042527) Full paper: [https://arxiv.org/html/2509.24988v1](https://arxiv.org/html/2509.24988v1) **Discussion Seed**: Previous works have suggested / used LLMs having self knowledge, e.g., identifying/preferring their own generations \\[https://arxiv.org/abs/2404.13076\\], or ability to predict their uncertainty. But paper claims specifically that LLMs don't have knowledge about their own *correctness.* Curious on everyone's intuition for what LLMs have / does not have self knowledge about, and whether this result fit your predictions. Conflict of Interest: Author is making this post.",
      "author": "Envoy-Insc",
      "timestamp": "2025-10-03T11:52:19",
      "url": "https://reddit.com/r/MachineLearning/comments/1nwoxqz/r_new_paper_llms_dont_have_privileged_self/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 30,
      "num_comments": 12,
      "upvote_ratio": 0.8,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nwfn4j",
      "title": "[R] Thesis direction: mechanistic interpretability vs semantic probing of LLM reasoning?",
      "content": "Hi all, I'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions: ‚Ä¢ Mechanistic interpretability (low-level): reverse engineering smaller neural networks, analyzing weights/ activations, simple logic gates, and tracking learning dynamics. ‚Ä¢Semantic probing (high-level): designing behavioral tasks for LLMs, probing reasoning, attention/locality, and consistency of inference. For context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack/frontend at first, but my long-term goal is to transition into backend. I'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact? Thanks in advance for your advice!",
      "author": "powerpuff___",
      "timestamp": "2025-10-03T04:50:44",
      "url": "https://reddit.com/r/MachineLearning/comments/1nwfn4j/r_thesis_direction_mechanistic_interpretability/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 10,
      "num_comments": 13,
      "upvote_ratio": 0.75,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nvxswc",
      "title": "[D] I‚Äôm looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation.",
      "content": "Imagine we train (or fine-tune) an LLM exclusively on physics texts up to 1904‚ÄîMaxwell, Lorentz, Poincar√©, Michelson‚ÄìMorley, etc.‚Äîand then ask it to produce a theory addressing the known tensions (e.g., invariance of c, simultaneity). The goal isn‚Äôt to re-derive Einstein verbatim or to validate anything in the lab, but to test whether an LLM can elaborate a novel, coherent theoretical structure from historically available knowledge. I‚Äôm interested in any domain, not just relativity: e.g., pre-quantum physics, pre-DNA biology, early group theory, early materials science, etc. What would count as ‚Äúon topic‚Äù: Pretraining from scratch or continual pretraining on a historically filtered corpus (time-sliced). Strong leakage controls: no access to post-cutoff texts; possibly knowledge unlearning. Evaluation focused on novelty + internal coherence (not experimental truth): e.g., CAS/proof-assistants for consistency, reviewers for ‚Äúhistorical plausibility.‚Äù Comparisons vs. baselines like RAG-only setups or modern LLMs that ‚Äúalready know‚Äù the breakthrough. Reports of failure modes (e.g., the model just paraphrases Lorentz/Poincar√©, or smuggles modern terms). Why I‚Äôm asking: I‚Äôve seen adjacent work (LLM-aided conjecture generation, symbolic regression discovering equations, RL systems finding new algorithms), but not a clean ‚Äúpre-discovery epistemology‚Äù experiment with strict temporal cutoffs. Tagging folks who might have seen or worked on something like this: u/hardmaru ¬∑ u/MysteryInc152 ¬∑ u/Qyeuebs ¬∑ u/StartledWatermelon ¬∑ u/Playful_Peace6891 ¬∑ u/SatoshiNotMe ¬∑ u/Ch3cks-Out ¬∑ u/NuclearVII If you know of: peer-reviewed papers, arXiv preprints, theses datasets/corpora curated by historical cutoff code or replication packages ‚Ä¶please share! Thanks in advance üôè",
      "author": "QuantumFree",
      "timestamp": "2025-10-02T16:04:04",
      "url": "https://reddit.com/r/MachineLearning/comments/1nvxswc/d_im_looking_for_papers_preprints_datasets_or/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 60,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nvj5hn",
      "title": "[D] Anyone here using LLM-as-a-Judge for agent evaluation?",
      "content": "I‚Äôve been experimenting with using another LLM to *score* my agent‚Äôs responses (accuracy / groundedness style) instead of relying on spot-checking. Surprisingly effective ‚Äî but only when the judge prompt is written carefully (single criterion, scoring anchors, strict output format, bias warnings, etc.) Curious if anyone else here is doing this? Any lessons learned? (I wrote a short breakdown of what worked for us ‚Äî happy to share if useful.)",
      "author": "Cristhian-AI-Math",
      "timestamp": "2025-10-02T04:13:48",
      "url": "https://reddit.com/r/MachineLearning/comments/1nvj5hn/d_anyone_here_using_llmasajudge_for_agent/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 0,
      "num_comments": 12,
      "upvote_ratio": 0.47,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1ntm0pf",
      "title": "[R] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
      "content": "Arxiv: [https://arxiv.org/pdf/2509.21880](https://arxiv.org/pdf/2509.21880) Huggingface paper: [https://huggingface.co/papers/2509.21880](https://huggingface.co/papers/2509.21880) I‚Äôve been working on improving the reasoning abilities of large language models, and I wanted to share something I‚Äôm really excited about. Reinforcement Learning with Verifiable Rewards (RLVR) is already a powerful framework, but I noticed a gap: current methods like GRPO only use problems where model responses differ in correctness. They completely ignore the so-called ‚Äúzero-variance prompts‚Äù ‚Äî cases where all responses receive the same reward. At first glance, these prompts look useless, but I started wondering if they actually contain valuable learning signals. That led me to develop **RL with Zero-Variance Prompts (RL-ZVP)**. Instead of discarding those prompts, RL-ZVP extracts meaningful feedback from them. It directly rewards correctness and penalizes errors without needing contrasting responses, and it uses token-level entropy to guide the advantage shaping. We evaluated RL-ZVP on six math reasoning benchmarks, and it delivered some really promising results ‚Äî up to **8.61 points higher accuracy** and **7.77 points higher pass rates** compared to GRPO. It also consistently outperformed other baselines that just filter out zero-variance prompts. I am happy to take comments in this sub and the HuggingFace paper.",
      "author": "SnooHesitations8849",
      "timestamp": "2025-09-30T00:03:37",
      "url": "https://reddit.com/r/MachineLearning/comments/1ntm0pf/r_no_prompt_left_behind_exploiting_zerovariance/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nrvlle",
      "title": "[P] Sample Forge - Research tool for deterministic inference and convergent sampling parameters in large language models.",
      "content": "Hi folks, I made a research tools that allows you to perform deterministic inference on any local large language model. This way you can test any variable changes and see for yourself the affects those changes have on the output of the LLM's response. It also allows you to perform automated reasoning benchmarking of a local language model of your choice, this way you can measure the perplexity drop of any quantized model or differences between reasoning capabilities of models or sampling parameters. It also has a fully automated way of converging on the best sampling parameters for a given model when it comes to reasoning capabilities. I made 2 videos for the project so you can see what its about at a glance the main guide is here https://www.youtube.com/watch?v=EyE5BrUut2o, the instillation video is here https://youtu.be/FJpmD3b2aps and the repo is here https://github.com/manfrom83/Sample-Forge. If you have more questions id be glad to answer them here. Cheers.",
      "author": "no_witty_username",
      "timestamp": "2025-09-27T21:43:40",
      "url": "https://reddit.com/r/MachineLearning/comments/1nrvlle/p_sample_forge_research_tool_for_deterministic/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nqosof",
      "title": "[R] Is there any research on using LLMs as Loss Functions?",
      "content": "Let‚Äôs say you were training a generative model for a task like summarization or answering questions. Would it be possible to feed that output into an LLM and ask it to assess the model‚Äôs effectiveness at performing the task and then maybe feed that output into a sentiment analysis model to obtain a score for how well the model did and have the model attempt to maximize that score?",
      "author": "Suspicious_State_318",
      "timestamp": "2025-09-26T09:41:29",
      "url": "https://reddit.com/r/MachineLearning/comments/1nqosof/r_is_there_any_research_on_using_llms_as_loss/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 0,
      "num_comments": 20,
      "upvote_ratio": 0.39,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nq856v",
      "title": "[R] ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
      "content": "We released ShinkaEvolve, a new state-of-the-art and fully open-source framework for program optimization, which we specifically designed to be easily integrated into any scientific codebase. Open source code:[ https://github.com/SakanaAI/ShinkaEvolve](https://github.com/SakanaAI/ShinkaEvolve) Technical report:[ https://arxiv.org/abs/2509.19349](https://arxiv.org/abs/2509.19349) Blog:[ https://sakana.ai/shinka-evolve/](https://sakana.ai/shinka-evolve/) You can start playing with ShinkaEvolve without even downloading any code, all inside a remote Google Colab instance:[ https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka\\_tutorial.ipynb](https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka_tutorial.ipynb) In our technical report, we show how ShinkaEvolve can be easily applied across different problem domains. On the canonical circle packing task, ShinkaEvolve discovers a new solution with state-of-the-art performance beyond the recent closed-source AlphaEvolve using only 150 program evaluations. We even apply ShinkaEvolve to small-scale LLM pretraining, discovering a new load-balancing loss for MoE architectures with remarkable stabilization properties. ShinkaEvolve also comes with a detailed and lightweight WebUI to monitor its discoveries in real-time!",
      "author": "Ereb0",
      "timestamp": "2025-09-25T22:23:34",
      "url": "https://reddit.com/r/MachineLearning/comments/1nq856v/r_shinkaevolve_towards_openended_and/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 23,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nplmr8",
      "title": "Apple Research Debuts Manzano ‚Äî a Unified Multimodal LLM",
      "content": "**üÜï What‚Äôs New** Apple research just introduced Manzano (Spanish for ‚Äúapple tree‚Äù üçè) ‚Äî a unified multimodal LLM that both understands images and generates them inside the same autoregressive loop. Instead of separate perception and generation models, one decoder predicts the next token ‚Äî text or image ‚Äî then renders pixels with an auxiliary diffusion decoder. The paper reports state-of-the-art results among unified models and competitive performance against specialist systems, especially on text-rich benchmarks. **‚öôÔ∏è How It Works** Hybrid vision tokenizer in front of the LLM: a single vision encoder feeds two lightweight adapters producing continuous embeddings for understanding and discrete tokens for generation. The unified LLM decoder accepts text tokens and/or image embeddings and auto-regressively predicts the next token; a diffusion image decoder turns predicted tokens into pixels. Three-stage training (pre-training ‚Üí continued pre-training ‚Üí SFT) on mixed text/vision data; the embedding table is extended with a 64K image-token codebook aligned by finite scalar quantization. **‚ú® What Makes It Distinct** Hybrid tokenizer, single encoder: understanding and generation tokens come from one encoder in a shared semantic space (no dual-tokenizer conflict). Decoupled roles: the LLM decoder handles high-level semantics; the diffusion decoder handles pixel fidelity ‚Äî letting each scale independently. Explicit scaling: LLM decoder scaled from 300M‚Üí30B params with steady gains; diffusion decoder scaled for stronger structure in human evals. **üìå Why It Matters** One model for ‚Äúsee + draw‚Äù ‚Üí simpler architecture, better language‚Äìvision alignment, easier product integration. Shared encoder + decoupled renderer ‚Üí a practical path to scale without sacrificing understanding (a weak point for earlier unified models). If these results generalize, future assistants that read, reason, edit &amp; generate in one loop could become the new default for multimodal work.",
      "author": "RIPT1D3_Z",
      "timestamp": "2025-09-25T03:31:35",
      "url": "https://reddit.com/r/MachineLearning/comments/1nplmr8/apple_research_debuts_manzano_a_unified/",
      "metadata": {
        "domain": "arxiv.org",
        "is_self": false,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 59,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1np483r",
      "title": "[D] Training smaller LLM for Agentic tasks.",
      "content": "So I have a specific use case, in which Deepseek-v3.1 works well, but it's simply too big and takes time to load on our GPU (everything runs locally in my organization, we have **16 H100 GPUs** and maybe about **8 more A100s**) .I use Ollama since I can‚Äôt keep VLLM loaded across all GPUs without hogging resources that others need. What I want is a **smaller model** that I can use for an **agentic task** mainly to work with a set of custom MCP tools I‚Äôve built. The biggest reason I want to build a model of my own is because I can get one hell of an education in the process, and since the hardware is already in-house (and mostly idle), I figured this is the perfect opportunity. But I‚Äôm not sure where to start: 1. Should I train a model from scratch, or take an existing pretrained model and fine-tune? 2. What base architecture would be a good starting point for agent-style tasks? If anyone can point me toward resources specifically focused on **training or finetuning models for agentic tasks**, I‚Äôd really appreciate it. P.S: I am currently using full precision deepseek-v3.1 (671B). I am thinking of a model which is about the size of gpt oss.",
      "author": "LifeguardNew6929",
      "timestamp": "2025-09-24T13:44:12",
      "url": "https://reddit.com/r/MachineLearning/comments/1np483r/d_training_smaller_llm_for_agentic_tasks/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 1,
      "num_comments": 6,
      "upvote_ratio": 0.54,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nok8yy",
      "title": "[P] SyGra: Graph-oriented framework for reproducible synthetic data pipelines (SFT, DPO, agents, multimodal)",
      "content": "**TL;DR.** We open-sourced **SyGra**, a graph-oriented framework for building *reproducible* synthetic data pipelines. Pipelines are defined as graphs (nodes = LLM calls/transforms/samplers; edges = conditional/parallel/loops). Two modes: YAML + CLI or Python library. Integrates with vLLM, HF TGI, Azure OpenAI, Ollama; HF-native I/O (streaming), provenance, schema-aware outputs. **Motivation.** High-quality LLM datasets are scarce, costly, and often sensitive; teams also need fine-grained control over task structure (SFT/DPO, tool use, multi-agent, multimodal). In practice, scaling ‚Äúnotebook pipelines‚Äù breaks down: you end up hand-wiring branching/looping flows, juggling multiple inference backends/APIs, and doing ad-hoc validation/schema checks‚Äîwithout resumability, sharding, or streaming. We wanted a **unified, reusable graph abstraction** that captures how data work actually happens (nodes/edges, subgraphs), automates **quality tagging** (heuristics + LLM-based scoring), and emits **schema-conformant, OASST-style** records‚Äîso teams can reproduce, audit, and evolve pipelines instead of rewriting glue code. **Design.** * **Graph model:** reusable subgraphs, branching, loops; deterministic configs * **Execution:** pluggable model clients (vLLM/TGI/Azure/Ollama), Triton-compatible * **Data I/O:** Hugging Face datasets (streaming), local files; schema &amp; metadata tracking * **Reproducibility:** explicit configs, seeds, artifact paths; CLI runs are fully logged **Use cases.** Bootstrapping SFT/DPO datasets; agent simulation &amp; tool-use evals; multimodal assembly (image‚ÜíQ&amp;A, audio‚Üítext) etc. **Links:** * Code (Apache-2.0) &amp; README: [github.com/ServiceNow/SyGra](http://github.com/ServiceNow/SyGra) * Paper (design rationale, examples): [arxiv.org/abs/2508.15432](http://arxiv.org/abs/2508.15432) * PyPI: [pypi.org/project/sygra/](http://pypi.org/project/sygra/) **Disclosure.** I‚Äôm part of the team. Feedback, issues, and PRs welcome.",
      "author": "zephyrzilla",
      "timestamp": "2025-09-23T23:19:45",
      "url": "https://reddit.com/r/MachineLearning/comments/1nok8yy/p_sygra_graphoriented_framework_for_reproducible/",
      "metadata": {
        "domain": "self.MachineLearning",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "MachineLearning",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "is_self": true
    },
    {
      "platform": "reddit",
      "post_id": "1nd4kbb",
      "title": "Making voice AI actually conversational requires rethinking the entire flow",
      "content": "Built voice control for our smart home devices that actually understands context and doesn't need wake words for everything. THE PROBLEM: Traditional IoT voice control is basically shouting commands at devices. \"Alexa, turn on living room lights.\" \"OK Google, set temperature to 72.\" It's functional but nobody wants to talk to their house like that constantly. WHAT ACTUALLY WORKS: Made the devices understand conversational context. Walk into a room and say \"too bright\" and it dims. Say \"actually a bit more\" and it adjusts. No wake words, no specific command syntax, just natural speech. The key was moving processing to the edge. Each device runs a lightweight model that understands context from the room it's in. Kitchen device knows \"start the timer\" means oven timer. Bedroom device knows \"too cold\" means adjust thermostat. IMPLEMENTATION: - Local wake word detection on ESP32 - Streaming audio to edge server on premises - Small LLM (3B params) running on local GPU - Device control via MQTT - Using agora for audio transport when controlling remotely The remote control part was interesting. When you're away from home, the app streams your voice commands through WebRTC to your local network, processes them on your edge server, then controls devices. Keeps everything private, no cloud dependency. Latency is around 200ms for local commands, 400ms for remote. Power consumption increased by about 15% per device but worth it for the natural interaction. Biggest surprise was how much context matters. The same command means different things in different rooms at different times. \"Turn it off\" at night in bedroom means lights. Same command in kitchen during cooking means timer. Anyone else working on conversational IoT? What's your approach to context awareness?",
      "author": "anikeithkumar",
      "timestamp": "2025-09-10T12:16:12",
      "url": "https://reddit.com/r/IOT/comments/1nd4kbb/making_voice_ai_actually_conversational_requires/",
      "metadata": {
        "domain": "self.IOT",
        "is_self": true,
        "over_18": false,
        "spoiler": false,
        "stickied": false
      },
      "subreddit": "IOT",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "is_self": true
    }
  ],
  "total_posts": 112
}