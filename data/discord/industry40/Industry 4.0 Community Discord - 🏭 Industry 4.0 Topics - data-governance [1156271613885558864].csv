AuthorID,Author,Date,Content,Attachments,Reactions
"745796393855352953","thedavidschultz","2023-09-27T00:55:31.8420000+08:00","FirstüòÇ","","üíØ (1),ü§£ (1)"
"745796393855352953","thedavidschultz","2023-09-27T00:57:06.7130000+08:00","@Brian Pribe and I are having a conversation about whether data should be modeled in a DataOps tool like HighByte or in a day hub like Libre. Version control can become a nightmare. But there are benefits to decoupling. Thoughts?","",""
"568913935147728896","zeratall","2023-09-27T01:07:32.1880000+08:00","I have a ton of thoughts lately around data governance and have some thoughts questions myself. Kind of related to the same topic and my line of thinking I posted in #üí¨-general I‚Äôm slammed atm irl doing some architecture work but I‚Äôll post a little later","",""
"745796393855352953","thedavidschultz","2023-09-27T01:08:00.1620000+08:00","Looking forward to it.","",""
"890244048739270656","brianpribe","2023-09-27T01:23:47.8220000+08:00","I guess one of my first questions to ask is who should be doing the data modeling and why? Why should a dedicated data modeling team be doing the work? Why should the frontline worker be doing the data modeling?","",""
"890244048739270656","brianpribe","2023-09-27T01:25:34.0330000+08:00","Frontline workers will be modeling, because they are trying to solve problems on the plant floor. A data modeling team will also be doing this, but with the added responsibility of managing the hierarchy and enforcing it.","",""
"794542235676180500","akoscs","2023-09-27T01:28:01.5500000+08:00","Fifth!","",""
"794542235676180500","akoscs","2023-09-27T01:29:57.3420000+08:00","In reality? Whoever knows best!","",""
"796223784650670110","dapomeranz","2023-09-27T01:35:48.6900000+08:00","You're getting at the answer to your own question. Which means you've thought about this a lot üòÜ 

Central team must own, document, and maintain models. But must take time and invest in educating the ""spokes"" at the business level to input and build for more specific problems. Great opportunity for career development. Especially as tools get easier to learn and use, anyone can build data models.","","üî• (1)"
"890244048739270656","brianpribe","2023-09-27T01:36:20.3970000+08:00","So anybody and everybody! The problem now is governing it. John Dyck from CESMII made an awesome point on integrating a single data point up to the MES level. It took 6 different software and over 1000 keyboard clicks to get there. I'm paraphrasing, but the same example can be applied to data modeling. How long do you think it would take to get one data model deployed in an enterprise? Highbyte can expodite much of this, but there is still a lot to be done in the database.","",""
"794542235676180500","akoscs","2023-09-27T01:37:46.2950000+08:00","If you think in monoliths, you are right. For microservices if development is decentralized then data modelling should also be decentralized. But governance is different from implemenation","",""
"568913935147728896","zeratall","2023-09-27T01:40:40.2890000+08:00","The way this conversation/channel is kicking off  is going is really getting me excited lol","",""
"890244048739270656","brianpribe","2023-09-27T01:45:20.0870000+08:00","Development must stay in their sandbox, but at production it's gotta conform. Microservices are great for the micro things that they do, but the data they produce to the rest of the ecosystem has to meet some MTRs (in regards to the data itself).","",""
"890244048739270656","brianpribe","2023-09-27T01:46:31.7840000+08:00","Timestamps and JSON format for example. Or even worse, conforming to medical or safety standards for reporting","",""
"568913935147728896","zeratall","2023-09-27T02:24:55.0800000+08:00","Totally agree, that‚Äôs what I was referring to in my other post. The OT domain requires flexibility to represent data to enable the floor. That being said consumers at the site or enterprise level need some structure  for those consumers to be able to navigate that structure.  

I think the central/monolithic data model approach is hard to maintain long term, but the alternatives solve some of these issues that also have ‚Äúdifferent problems‚Äù 

For the these IT/OT architectures I think there is a real question around even some of the basics like, who becomes the data stewards, domain stewards, who manages the data management plan. Most of these best practices for governance are for traditional IT which I think fall flat in OT","",""
"890244048739270656","brianpribe","2023-09-27T02:29:37.3660000+08:00","The importance can't be underestimated as well. If we are designing our enterprises to be data-driven (data informing and making decisions), then there must be a dedicated role for this at the C-level.","","üëçüèª (1)"
"568913935147728896","zeratall","2023-09-27T02:30:16.2930000+08:00","Totally, imho Data needs to be seen as a company asset and developed and managed like any other end product.

I believe once enterprises see data/information as a product itself instead of some process bi product, that‚Äôs what will drive some of these transformation things like org restructuring,  etc. then again that‚Äôs my personal take on it lol","",""
"890244048739270656","brianpribe","2023-09-27T02:34:56.9600000+08:00","Yuuuup. I think it needs to be equated to dollar signs. Just as money is the fuel that makes things happen, so it is too with data. Walker in mastermind had a module dedicated to restructuring the C-level and I never quite got it until more recently.","",""
"745796393855352953","thedavidschultz","2023-09-27T03:01:30.0280000+08:00","It is the balance of ‚Äúthou shalt‚Äù and ‚Äúthou may.‚Äù I firmly believe there should be an enterprise enforced data model that facilitates corporate decision making. The sites should then be able to expand the model based on their needs. For example, an Ignition UDT can reference a parent UDT. In this case the parent is the enterprise model and the child is the site. There are many ways to accomplish this task.","",""
"568913935147728896","zeratall","2023-09-27T03:07:54.9060000+08:00","Yeah I totally agree, I see the need and imho there needs to be something in the architecture that facilitates the need. I‚Äôm just not sold on the ‚Äúhow‚Äù. I think a central data model becomes hard to manage at scale. While I like UNS, I also see issues on distributed governance. That‚Äôs why I said they both have pros and cons but the cons are just different.","",""
"745796393855352953","thedavidschultz","2023-09-27T03:08:33.2940000+08:00","The devil is always in the details.","",""
"745796393855352953","thedavidschultz","2023-09-27T03:10:23.7030000+08:00","Perhaps a DataOps team comprised of several stakeholders. There would be an enterprise team, site teams, and a liaison between them.","","üëç (1)"
"568913935147728896","zeratall","2023-09-27T03:12:36.0160000+08:00","That‚Äôs exactly my thoughts and part of the data governance strategy I‚Äôve been thinking of the past few weeks ever since @GeoffNunan brought up the data governance question, it made me rethink my approach and go down the rabbit hole lol","",""
"894527802316046366","nickn5549","2023-09-27T05:50:15.2600000+08:00","any data is better than no data...to start with","",""
"783917475128410112","geoffnunan","2023-09-27T09:08:40.4250000+08:00","Wouldn't it be fantastic if a bunch of people with 20 plus years experience in all the different manufacturing domains got together and captured their knowledge so the rest of us could learn from their experience and we didn't have to re-invent the wheel every time.","","üëç (3)"
"894527802316046366","nickn5549","2023-09-27T12:52:22.6420000+08:00","But the wheels have better tires every time, if we disregard their squarish shapes","",""
"844671073827291156","yasirtuncer","2023-09-27T14:04:24.0080000+08:00","All the experience in the world would not solve this though. The reason is the mass customization of assets. For example we work with T√ºrkiye's biggest automotive OEM and they have tens of different CNCs for machining transmission and engine parts. 

They wanted to create a common AI model for estimating the remaining useful life of cutting tools. We contextualized from several data sources like 500+ tags/inputs to feed the model. They completed their feature engineering and selected 20+ critical parameters. However when they scaled to other CNCs there were some assets which do not generate some of the specific parameters and there were some cases where adding the input from a measurement system or a probe enhanced the accuracy of the model. 

So it turned out even though it is centrally managed, the flexibility on modeling is a must and when the overall heterogeneity is considered the exception becomes the rule. 

There are some context which is applicable despite the heterogeneity like cycle time, breakdowns, OEE etc. however when we extend this to AI/ML or solving specific problems on the shop floor it is impossible to have a central model where you have child - parent relations or derivatives etc. 

One of the real challenge on data governance lies when you give flexibility to shop floor to solve their problems and find out two different site is trying to solve the same problem with different data models and you try to centralize those","","üíØ (1)"
"783765494275506256","autopsias","2023-09-27T14:47:59.6010000+08:00","Let me through in another subject that can very quickly also be transformed in a rabbit hole. For any Data Governance to work you need data lineage, so you can assert what was changed by whom and when, and you can capture errors and improve data quality.","",""
"783765494275506256","autopsias","2023-09-27T14:50:33.5210000+08:00","There are a number of tools to do this semi automatically but they typically work well within the realm of a data lake for instance. A single source of truth which you can follow most data ops. Problem is when you start to want to do this through your UNS broker, data lake, etc. you need to have a fairly complex metadata system so you can capture the necessary details from one node of the ecosystem to the others. Anyone has elegantly solved this conundrum?","",""
"867075936054149191","rickbullotta","2023-09-27T19:49:23.7050000+08:00","If ""data is the new oil"", it often needs a sh*t ton of refining to make it usable...","","üòÜ (3),ü§£ (1),üòÇ (2)"
"890244048739270656","brianpribe","2023-09-27T21:02:38.5900000+08:00","Damn right it does. If you are going to use liquid dinosaur carcasses to power the world, you need to heavily invest into processing it down to a usable fuel. Same story goes for data. You have to aggregate, contextualize, and democratize to reap the benefits. Luckily, it doesn't require as much overhead.","","üíØ (2)"
"745796393855352953","thedavidschultz","2023-09-27T23:08:46.8490000+08:00","Good thing the US has extensive experience in dealing with the heavy crudes.","",""
"844671073827291156","yasirtuncer","2023-09-28T00:09:04.0620000+08:00","@Ricardo Carvalho Let's have a chat on what we do and how we visualize that data lineage üòä","","üëç (1)"
"753688565807841492","ravil1","2023-09-28T02:28:55.6380000+08:00","@yasirtuncer, in OPC UA objects have mandatory properties and optional ones. Probably it worth for you to look at how information and object model is defined in OPC UA companion specification for CNC systems: https://opcfoundation.org/developer-tools/documents/view/216 Probably you need to register and login to download, that is easy and free.
It was created by many companies working on this domain, my guess is they cover wide range of different CNC machines. So you could leverage their experience I guess.

You don't need to use OPC UA in your projects, just can use the OCP UA companion specifications as a reference. Or even use naming conventions defined in the specification as a part of topics in the UNS, this way you don't need to re-invent the wheel and it can be easier to come to some common design used by multiple sites/teams.","",""
"794542235676180500","akoscs","2023-09-28T03:50:04.9470000+08:00","500 inputs for cutting tool RUL? oh wow. Do you know how you collected these what was the comms protocol? Any additional high freqency data aqusition?","",""
"794542235676180500","akoscs","2023-09-28T03:51:21.2010000+08:00","Elegantly? no one.","",""
"794542235676180500","akoscs","2023-09-28T03:52:40.7450000+08:00","If you try to use OPC UA for cutting tool RUL the project is doomed form the start. You need high frequency data with time determinism.","",""
"844671073827291156","yasirtuncer","2023-09-28T03:55:03.8410000+08:00","Please define elegantly? At MaestroHub we have the complete lineage and how it is contextualized with different data sources in this fashion. Also you can access the logs as well üôÇ

The image above is for example how is the cycle time is used for different instances and data flows","https://cdn.discordapp.com/attachments/1156271613885558864/1156680426560176138/image.png?ex=68df8157&is=68de2fd7&hm=ff5d2554deb9b39150f247e7752e9aab824016c6e1c79ab606481140959727a4&",""
"844671073827291156","yasirtuncer","2023-09-28T03:58:08.4900000+08:00","This works for some CNC manufacturers like DMG Mori CTX Beta with Siemens 840 the problem is you can implement a Renishaw probe which will not be covered in those common OPC models. Most of the CNCs do not even publish their NC parameters like feedrate axis movements etc.","",""
"753688565807841492","ravil1","2023-09-28T04:01:02.0730000+08:00","I was not suggesting to use OPC UA as a communication protocol, although curious what are number of tags and frequency. 
I was just suggesting to look at the CNC companion spec to see if something can be borrowed to build the information model.","",""
"844671073827291156","yasirtuncer","2023-09-28T04:02:30.1290000+08:00","Siemens Edge with MQTT another edge device to read from NC registrar and publishes via MQTT. 
There are 24 different tools so you need to track those in different topics 
For OPC you are 100% right, with these high-frequency use cases OPC is too heavy and slow like a slowpoke pokemon üôÇ","",""
"794542235676180500","akoscs","2023-09-28T04:07:00.8280000+08:00","So you had to go through the Siemens Cloud or they finally lifted the artificial rate limiter in the Siemens Edge reverse proxy? Even the NC is too slow for motor current related data, as the current controller loop is in the drive and the profinet cycletime is not enough to capture meaningful current values. But if you have the current values at high frequency (10kHz++) and you can contextualize them wiht motion data form the NC with the interpolator cycletim then you have the basis for tool RUL, otherwise not chance of generalization I agree!","",""
"753688565807841492","ravil1","2023-09-28T04:07:48.8610000+08:00","Did you actually check in the OPC UA CNC specs that those exact parameters not defined there? I see some parameters about feedrate there.","",""
"794542235676180500","akoscs","2023-09-28T04:09:12.2120000+08:00","yes, but you get them with OPC UA at a 100ms cycletime level. You need it at a 2ms level. Motor currents you need at a way higher rate.","",""
"844671073827291156","yasirtuncer","2023-09-28T04:09:17.7890000+08:00","Yeah actually I checked it out I was in Hannover last week and had these discussions with VDMA as well. The problem is even though feedrate is defined in OPC specs the CNC does not publish them","",""
"794542235676180500","akoscs","2023-09-28T04:11:24.1400000+08:00","You can get feedrate out of a 840d sl (maybe not from the pl). Not according to the OPC UA comp spec, because their implementation pre-dates the spec by years, byt you can get feed rate. Not with the frequency you want for this application though.","",""
"794542235676180500","akoscs","2023-09-28T04:12:37.5310000+08:00","@Zach E How are we doing regarding hijacking the thread? üôÇ","","ü§£ (1)"
"753688565807841492","ravil1","2023-09-28T04:18:01.5850000+08:00","I can see that by default often OPC UA Servers revise sampling interval to some value like 100 ms fastest, but it can be modified to microseconds. Depends on how the server is implemented.
So you say that the PLC can publish data with sub-millisecond rate over MQTT, but cannot do that over OPC UA? I would expect opposite.
This is interesting topic for me,  our product in the current version was not allowing to configure requested sampling rates below 1 ms, but in the next version it will support microseconds. And returned by the server revised values will be available to be displayed in the GUI,
During tests we realized that often demo servers revise it to 50 ms or 100 ms.","",""
"844671073827291156","yasirtuncer","2023-09-28T04:18:37.0970000+08:00","Some of the critical parameters that we couldn't get via OPC, even with a lower (reasonable) frequency
- ActiveToolpos
- Driveload
- Singleblock
- Drivepower","",""
"844671073827291156","yasirtuncer","2023-09-28T04:26:14.5190000+08:00","The rising edges are missed but as you said it could be due to server configurations","",""
"753688565807841492","ravil1","2023-09-28T04:28:15.1150000+08:00","What was the reason - they just do not exist in the server address space? In that case it is that specific server implementation details. 
But, again, I was just suggesting to look at the OPC UA CNC companion spec as a reference to see how information model is designed there. 
Probably in newer versions the vendor can add these missing parts. 
I was more intrigued by the statement that data cannot be received over OPC UA not because it is not available, instead, the server cannot handle the load, while with MQTT it can handle. 
If data was not available at all, it means it is unknown if the PCL could handle that load over OPC UA.","",""
"753688565807841492","ravil1","2023-09-28T04:32:08.0030000+08:00","Hi @akos, can you confirm specific case when you could not get desired data with this rate over OPC UA because it was too slow, and at the same time there was no problem getting the same data with sub-millisecond rates over MQTT?","",""
"794542235676180500","akoscs","2023-09-28T04:35:34.5750000+08:00","The OPC UA server does not do the sampling. It in turn is connected to the NC. The OPC UA server runs in the non real time part of the NC. You have no chance of getting anything useful with high frequency regardless of sampling rate because the variables which the OPC UA ‚Äúsamples‚Äù are not updated. Furthermore even if they would be you have no time determinism on the sampling since it runs in a non real time environment. The Siemens Edge has a different connection to the NC and (I assume) an internal buffer for high(er) frequency variables. The reason I assume there is a buffer because there is a limit to how many high freq variables it can record.","",""
"794542235676180500","akoscs","2023-09-28T04:38:11.9060000+08:00","No, it is not about MQTT is faster, it is about how the subsystems that feed the OPC UA server and MQTT get their data. One step before it is OPC UA or MQTT. It is not about MQTT it is about the mechanism Siemens ises internally to connect their edge device to their NC. Maybe it is the S7comm protocol with some special NC variable access I do not know.","",""
"794542235676180500","akoscs","2023-09-28T04:39:54.2120000+08:00","Again, it is not a data modelling problem the spec does not help at all","",""
"794542235676180500","akoscs","2023-09-28T04:41:10.4790000+08:00","Probably the vendor will not add the missing parts since they are focusing on their new controller not on the 840d anymore.","",""
"753688565807841492","ravil1","2023-09-28T04:51:36.1990000+08:00","Thank you for sharing details, akos!
Yes, I know that the server might not be doing sampling itself. In terms of OPC UA, sampling interval in subscriptions and monitored items is the time resolution desired by the client. Sometimes it can be under server control what actual sampling intervals can be, but often depends on underlaying data source and completely defined at that level, like in your case how often data values are updated in the server side seems depends on the NC.
So in your case there is no OPC UA Server which conforms to your requirements. It is not about that OPC UA protocol in general is not suitable for the task. Real time part I think can be resolved by time-sensitive network support, but again there is not server exists yet for this specific use case I guess. But companies are working on this direction I assume.","",""
"844671073827291156","yasirtuncer","2023-09-28T04:55:58.1150000+08:00","Back to data modeling üôÇ 
The standards bring some common ground I agree with that but it never is the complete picture when we are talking about assets which have mass customization. I worked several years closely with Fraunhofer and even though never publicly stated they admitted it cannot be achieved with a common data model when we discuss Asset Administration Shell concepts

Because standards require true collaboration, sacrifices, trade-offs which is almost impossible to achieve when there are so many competing companies for the same market. OPC has been trying to establish common data models for 5+ years and my assumption is it will take at least another 5 more years for the industry to accept those models.","",""
"753688565807841492","ravil1","2023-09-28T05:00:25.0000000+08:00","Looks like there is no OPC UA Server yet supporting this CNC companion spec for your specific CNC. Quick check shows that for example CncDrive has  properties ActLoad, ActPower, I guess these are Driveload and DrivePower in your list.","",""
"753688565807841492","ravil1","2023-09-28T05:04:28.0190000+08:00","Even if the standard is complete, I think often it is additional overhead to follow it up the the letter.
With MQTT/UNS you can quickly design your own company-scope standard and deliver with less effort in shorter time. 
Publish Whatever Whenever to Whoever approach gives you this freedom.","",""
"844671073827291156","yasirtuncer","2023-09-28T05:10:42.7540000+08:00","If I recall correctly there was a video between Walker and JP to discuss the data modeling.
UNS with dataops gives you that flexibility. Do you think just a broker is enough to solve this problem?","",""
"753688565807841492","ravil1","2023-09-28T05:27:33.6920000+08:00","If you are about problem with different site teams are designing UNS on their own ways for the same equipment, I guess this could be solved just by administrative methods. 
If equipment are different types with different parameters, then each site broker can have its own topic structure slightly different than the company-wide standard, and then you can perhaps adjust structure / normalize it when you push from the local site brokers to the higher level enterprise broker maybe.
I mostly focused on moving data from OPC UA to MQTT, SQL, InfluxDB, Kafka types of destinations.","",""
"919232821476851783","petrusaraujo","2023-09-28T06:23:27.9290000+08:00","In that context of administrative methods, what would be the best practices to govern data models when it comes to define a baseline to compare with actual results? For example: crew size (one site could run with 2 HC and another site can run the same machine with 3HC due to something independent from the machine), labor hours, machine speed by design, etc","",""
"568913935147728896","zeratall","2023-09-28T10:19:24.8150000+08:00","Totally agree, especially with the last paragraph, that‚Äôs been my view point on applying a traditional master data model approach across the enterprise. Have you found anything that works better that allows the floor to manage their data but still allow integration for analysis at the enterprise layer?","",""
"894527802316046366","nickn5549","2023-09-28T14:59:17.5520000+08:00","and less pollution...","",""
"844671073827291156","yasirtuncer","2023-09-28T15:47:02.0130000+08:00","We are still trying to solve this challenge, we tried meta data matching, entity ownership but still not solved completely. At the end of the day I believe even though we provide technological solutions the shop floor should talk to each other regularly even though they are on different sites. 

From my consultancy experience best way to solve this challenge is creating working groups that focuses on similar problems and let them have their bi-weekly or monthly plannings together. we should not omit the power of simple communication üôÇ","","üëçüèª (3),üëç (2)"
"894527802316046366","nickn5549","2023-09-28T17:29:15.6610000+08:00","Many times the message is benevolent but attitude generates conflict....on the other hand talk is free and if is channeled properly helps generating great specifications and pinpoints the biggest problems.","",""
"568913935147728896","zeratall","2023-09-28T21:50:18.8110000+08:00","Damn I‚Äôm on the same quest was hoping someone smarter then me could provide some insight. I really do agree with taking advantage of simple communication there is some low hanging fruit that while not a perfect solution makes a lot of headway to align across sites with minimal effort.","",""
"1139269147126153216","suran23.","2023-09-29T07:11:15.0040000+08:00","We are trying to muster some of that expertise and present that in our upcoming FREE VIRTUAL event ""Do More With Data"" sessions. Deep-dive into the tech stack, evolution of Shadow IT, Pitfalls and Best Practices, What worked, what did not (only real incidents).....Innovative approaches to UNS and more. Hope to see you there and if you want to speak, that's even better. We need real life examples and trend setters.   Register: https://events.hubilo.com/do-more-with-data-summit/login Apply to be a speaker: https://forms.office.com/Pages/ResponsePage.aspx?id=UmOM9f8PXUujCyk44PRhUghK3PIBvBdAlRNGjLDhhBZUQ05ZVjNUNEhRRVdDUVQxOVlUWjM4SzZNUi4u","",""
"756247672637358181","sherylmccrary","2023-10-07T14:18:53.9080000+08:00","Pinned a message.","",""
"756247672637358181","sherylmccrary","2023-10-10T05:24:00.4430000+08:00","Is this happening? i don't see any topics or speakers on the agenda yet.","",""
"1139269147126153216","suran23.","2023-10-10T05:27:38.2460000+08:00","@SherylM Yes, it's taking place as scheduled and speakers, topics, agenda are getting flushed and finalized as we speak. Please let me know for anything.","","üòÄ (1)"
"919232821476851783","petrusaraujo","2023-10-11T06:04:24.0820000+08:00","On manufacturing dashboards, how have been your governance approach? Free-for-all, adaptive governance or dictatorship?","",""
"475955754788978688","jdingus","2023-10-17T07:15:21.7920000+08:00","Did anyone get the definition of data governance copied down from Walkers mastermind?  Couldn't type fast enough lol.","",""
"1073632885153730621","vaughnturner","2023-10-18T06:48:24.8280000+08:00","Definition of Data Governance: A set of processes, roles, policies, and metrics to ensure the effective and efficient use of data and information to achieve an organization‚Äôs digital strategy.","","‚ù§Ô∏è (3)"
"475955754788978688","jdingus","2023-10-18T18:25:28.0930000+08:00","Thanks Vaughn!","",""
"1073632885153730621","vaughnturner","2023-10-18T21:27:52.3350000+08:00","My pleasure.","",""
"742752935947010118","busch1974","2023-10-31T22:43:30.9990000+08:00","doug albright","",""
"796223784650670110","dapomeranz","2024-01-04T02:22:19.8490000+08:00","Did a YouTube video on dbt and the general movement of enabling data analysts to work like software engineers. I think this stuff is critical for larger organizations to have meaningful data warehouses and bi reporting, and hopefully this video helps others understand.
https://youtu.be/RMtvzeRJmHM?si=NZ_WBnJRSz-Hzhno","","üî• (2),üëè (4)"
"898217314741280828","hobbes1069","2024-01-04T06:00:45.4750000+08:00","Nice video!","",""
"471819507594297384","enrimarini","2024-01-05T06:18:53.6210000+08:00","I always wondered what platforms were out there to document data transformation  & track history outside of forum-lite tools like Confluence and SharePoint site wikis. I've typically seen devs use rapid website building tools like Docusaurus, in the absence of commercially licensed forum software, but they've always been an afterthought and lack much of the programmatic capabilities of solutions like dbt. 

Personally, I agree - no more proprietary tools to track documentation. All hail the markdown gods and git.","",""
"796223784650670110","dapomeranz","2024-01-05T06:25:34.7310000+08:00","Thanks guys! I am curious if others have tried dbt on historians or more continuous data sources. My experience has been all ERP/CRM level data.","",""
"796223784650670110","dapomeranz","2024-01-05T06:35:14.1900000+08:00","Like how could these concepts be applied to a plant that wants to cleanly trace and document the way that 10 unique areas calculate OEE?","",""
"471819507594297384","enrimarini","2024-01-05T06:37:29.8770000+08:00","Although I haven't seen a dbt implementation of such, I absolutely see the value in doing so & I can clarify that the way it was handled even here during the MES Bootcamp build-out was essentially commenting the Jython code inside of Ignition. There was no real robust tracking of who/what/where/when/why/how of data transformation in the MES environment other than the Jython scripts loaded onto our Ignition instance.","","ü§î (1)"
"898217314741280828","hobbes1069","2024-01-06T00:29:59.2630000+08:00","I don't have a good solution either, currently I document everything using Markdown in VS Code and an Azure DevOps instance.","",""
"898217314741280828","hobbes1069","2024-01-06T00:32:30.4600000+08:00","And to be clear, I use it for EVERYTHING... I keep notes / howtos because when something breaks a year from now I won't remember exactly what I did. I use it for edge device config backups if they can be exported to text (JSON, XML, etc), and exports of my Node Red flows.","",""
"471819507594297384","enrimarini","2024-01-06T01:04:51.8630000+08:00","I'm willing to bet pretty much everyone does just this. Like Dan mentions in his video, while I too commend people for doing this, it's really not good enough. 

Scripts change all the time. Git-diff is ok but does nothing to capture architectural design choices on underlying assumptions explaining why the data was changed/set-up the way it was. It's worthwhile investing in developing an environment to capture such assumptions & transformation lineage. 

Only the software gods can fathom the countless hours we have all wasted trying to decipher someone else's work. I personally would love to hear from any master data and/or dedicated data governance teams on their experience in using dbt-core. 

We have all sorts of environments to capture content, manage its history, version control of code, track tasks in project management boards, and so on. We say data is the most valuable commodity yet I would argue we seem to have neglected the most important aspect of all when it comes to data - creating a programmatic & scalable method to track data transformation lineage.","","üôå (1)"
"867075936054149191","rickbullotta","2024-01-06T20:04:31.9810000+08:00","Idiot","","üòÇ (2)"
"471819507594297384","enrimarini","2024-01-13T09:54:47.7860000+08:00","Complete spam, already reported. Urging everyone else to also report this person.","",""
"796223784650670110","dapomeranz","2024-03-11T23:45:04.6420000+08:00","https://www.youtube.com/watch?v=poxmV4_gFUM","",""
"796223784650670110","dapomeranz","2024-03-11T23:47:28.7410000+08:00","Longer form content but I think this is a very robust conversation about data governance, business intelligence, and AI for data. I definitely have a transactional data background so I am curious how much of this applies in the continuous world. @Jeff@Flow seems to have the most relevant content for machine data.

This was a prerecorded video for Simio Sync 2024.","",""
"475955754788978688","jdingus","2024-03-28T02:20:15.7180000+08:00","Anyone ever used Kestra.io in the I4.0 stack?  

Has tons of IIoT protocol connectors.  Here‚Äôs a quick LLM summary of it.

Kestra is an open-source data orchestration platform that allows users to automate complex workflows and integrate with their existing data stacks to enhance data processing efficiency and speed. It supports Infrastructure as Code practices, allowing for declarative workflow creation and management, and offers a user-friendly UI for workflow orchestration. Kestra enables the definition of workflows in YAML and supports various programming languages for scripting within workflows. It‚Äôs designed for scalability, allowing users to run workflows anywhere without a single point of failure, and provides a rich plugin ecosystem for extended functionality   .","",""
"795725198980677734","diederik9434","2024-03-28T18:42:42.9270000+08:00","Hi guys, if you have sensor data in the UNS, what context do you typically add to make the data more valuable?","",""
"230441548653789184","r.pop","2024-03-28T21:20:25.6580000+08:00","Location, Machine, Process Area, Unit of Measure, Rate of Collection, etc..","",""
"766684226455207996","bright_hummingbird_31342","2024-03-29T04:21:07.5950000+08:00","Think less about publishing into the UNS. Think more about what a line business user would want to consume from the UNS and work back from this.","","üëç (4)"
"898217314741280828","hobbes1069","2024-03-29T09:11:58.1830000+08:00","Yes, thinking about how you would want to consume data helps you determine what (and how) you should publish it.","",""
"894527802316046366","nickn5549","2024-03-29T12:46:29.8470000+08:00","IP, hostname, OS, firmware, software version, hardware type, SN....","",""
"795725198980677734","diederik9434","2024-03-29T15:41:11.0580000+08:00","@js  Ok I do agree with this but that's kind of the issue. For the PoC i'm working on (energy usecase) i have a solid idea what to add as metadata but we're also working on a general UNS at the meantime. For energy data it makes sense what to add as meta but in general that's more difficult. I dont want to miss any data now that could be valuable for general usecases later on. (otherwise my datamodel is directly inconsistant across usecases)","",""
"795725198980677734","diederik9434","2024-04-04T04:03:43.5530000+08:00","Hi everyone!
Has anyone experience pushing OT/automation to good data governance? How do i make sure each sensor/PLC is supplying the same data? I would like to allow the OT engineers to map their data to our standard data models. I know highbyte is super fit for this but i need a cheaper solution. Any suggestions? üôÇ","","üëç (1)"
"795725198980677734","diederik9434","2024-04-04T04:14:59.0650000+08:00","(so something to do decent DataOps with)","",""
"1214242167640424619","zack.scriven","2024-04-06T22:18:03.6480000+08:00","Have you looked into litmus?","","üëç (1)"
"230441548653789184","r.pop","2024-04-08T07:48:20.3630000+08:00","Check out Flow Software. @Jeff@Flow can probably help.","",""
"742752935947010118","busch1974","2024-04-08T20:16:27.8740000+08:00","https://powerbi.microsoft.com/en-us/blog/announcing-the-public-preview-of-database-mirroring-in-microsoft-fabric/","",""
"742752935947010118","busch1974","2024-04-08T20:17:23.5200000+08:00","We are looking at this for low cost ETL.  Ignition Transaction Groups to DB with DB Mirroring to Fabric","",""
"230441548653789184","r.pop","2024-04-08T21:32:06.0630000+08:00","I would look at Snowflake as well. Especially if your using Ignition. https://cirrus-link.com/iot-bridge-for-snowflake/","",""
"1184762752758194199","rma002","2024-05-05T01:31:38.6210000+08:00","Have you found any cheap solution yet ? @Diederik","",""
"795725198980677734","diederik9434","2024-05-05T06:15:53.3330000+08:00","to be honest; i have found nothing yet that really solves this issue.
I've been doing IIoT for a few (4 years) now and have done a few projects (very big (>100000 tags) to very small size (+-50 tags)) and this is the single biggest issue i've seen everywhere","",""
"795725198980677734","diederik9434","2024-05-05T06:15:57.3880000+08:00","very hard to fix","",""
"795725198980677734","diederik9434","2024-05-05T06:16:41.5400000+08:00","i'm a strong believer in highbyte but have not been able to test it in a full scale project","",""
"795725198980677734","diederik9434","2024-05-05T06:17:37.7240000+08:00","i like united manufacturing hub as a free solution that forces a certain topic/message structure for the UNS; this fixes 10% of the issue already","",""
"795725198980677734","diederik9434","2024-05-05T06:18:11.9680000+08:00","they are working on an extended implementation of a ""data hub""/governance; i have high hopes üòÑ","",""
"795725198980677734","diederik9434","2024-05-05T06:19:42.0020000+08:00","i do believe goverance is the key to a good IIoT story","","üíØ (2)"
"795725198980677734","diederik9434","2024-05-05T06:20:34.9570000+08:00","and it's everywhere; if someone handles it's projects/data with care, dreams can come true
no care => no ROI","",""
"795725198980677734","diederik9434","2024-05-05T06:20:58.9630000+08:00","@Rma","",""
"795725198980677734","diederik9434","2024-05-05T06:23:35.7670000+08:00","the technology is there; its ""the human aspect"" of IIoT that's lacking in our sector
That's why we need preachers like @Walker Reynolds","",""
"844671073827291156","yasirtuncer","2024-05-06T17:10:24.0870000+08:00","@Diederik Have you looked into MaestroHub? We will be releasing a land&expand version before the end of Q2 and there are quite a lot of features for governance. What are the problems that you are trying to solve with dataops besides standardization of data models? Where do you want to solve this issue at the source before ingesting to UNS or after publishing to UNS and sharing with other applications? There are several alternatives where and when you apply dataops, so it would be great if you can elaborate a few details.","","üëç (3)"
"795725198980677734","diederik9434","2024-05-06T17:12:15.8580000+08:00","Didnt know it; looks good on first sight!","",""
"795725198980677734","diederik9434","2024-05-06T17:12:17.3080000+08:00","Thanks","",""
"822206286736523294","jeff_flow","2024-05-24T03:41:20.1660000+08:00","Would encourage a look at Flow, this is right in our wheelhouse. Using the template feature within Flow lets you deploy projects that are centrally managed and every part of the template can be governed. Full auditability and versioning. If you would like a demo, happy to help, just DM me.","","üíØ (3)"
"798219851352899625","8elahr","2024-05-24T04:52:05.9790000+08:00","Do you have a link for me?","",""
"1214242167640424619","zack.scriven","2024-05-24T07:07:16.1920000+08:00","Thanks Jeff!","",""
"1214242167640424619","zack.scriven","2024-05-24T07:07:33.8860000+08:00","https://www.flow-software.com","",""
"1214242167640424619","zack.scriven","2024-05-24T07:13:39.0290000+08:00","@Jeff@Flow 2 things. 

1. Would love to get a Flow Software deep dive on the mastermind curriculum schedule. 
2. ProveIt! 2025 Industry 4 Community Conference","",""
"1214242167640424619","zack.scriven","2024-05-24T07:14:36.8310000+08:00","that's the sociologist and educator in walker","",""
"822206286736523294","jeff_flow","2024-05-24T21:09:53.5020000+08:00","You are welcome to use this link - https://www.flow-software.com/download, it will provide a 90 day demo, I can extend the license so that you do not lose your work when the 90 days runs out and continue with another 90 days, and so on.","","üëè (3)"
"822206286736523294","jeff_flow","2024-05-24T21:11:10.2110000+08:00","Would love to do both. My calendar link - https://meetings.hubspot.com/knepper/discussion","","‚ù§Ô∏è (1)"
"798219851352899625","8elahr","2024-05-25T01:03:00.8150000+08:00","Hi Jeff, 
thanks for the offer üëåüèª
I‚Äôll come back to it soon. üí™","",""
"1050076217329586368","volkan0341","2024-06-06T00:58:54.4140000+08:00","I don't know if it's cheaper, but I recommend you check out our newly released Proxus: https://www.proxus.io/ I can provide a demo license if you'd like.","",""
"1214242167640424619","zack.scriven","2024-06-06T23:41:50.1110000+08:00","thanks for taking the meeting! That was excellent to discuss what you guys are working on at Flow.","",""
"138867775354437632","jonathan08947","2024-06-09T09:22:03.5620000+08:00","FYI, I'm having trouble clicking on links in your website.  I'm using Chrome on Android","",""
"1050076217329586368","volkan0341","2024-06-10T03:32:56.6430000+08:00","Thanks for the warning Jon. The product and website are very new, and we will fix this issue as soon as possible.","",""
"867075936054149191","rickbullotta","2024-06-18T22:28:28.6990000+08:00","This is super interesting - blend cold storage + hot/warm data easily.

https://blog.paradedb.com/pages/introducing_lakehouse","","üî• (2)"
"1248229600853758073","sptha_27377","2024-06-21T16:51:03.6300000+08:00","Hi All
I'm strugeling with a HighByte flow, maybe someone can help me out. My input is an SQL query, returning multiple lines of orders from an ERP. I like to transfer it to an MQTT output, where I have the Breakup Arrays turned on and my topic looks like this: SPT/SG/ERP/Orders/{@this.OrderNumber}, however, I'm never able to get the OrderNumber parameter expanded. In MQTT Eplorer I see the image as attached. I guess I'm missing something realy simple here...","https://cdn.discordapp.com/attachments/1156271613885558864/1253633272450387998/image.png?ex=68dee5b7&is=68dd9437&hm=e1ceb3b2f6c0ef73405748ce3b1eb3c6d3e921fcf4567234db26ff946f6b723b&","üî• (1)"
"867075936054149191","rickbullotta","2024-06-21T19:59:19.0450000+08:00","I‚Äôm sure @js and @aronsemle can help.","",""
"456226577798135808","Deleted User","2024-06-21T19:59:21.2320000+08:00","I have encountered similar problems before. While I don‚Äôt know the exact solution, I usually resort to pipelines where I can interrogate the input JSON and then add a breakup block to analyze the content of the JSON again before outputting with a write or write-new block. Sometimes, having a breakup block within the pipeline and also on the Connection Output solves my problem. The point is that pipelines, with their event recordings and analysis capabilities, are very handy.","",""
"910205937409732628","ryan.clavette","2024-06-21T21:58:05.3970000+08:00","Hi @Herbert  - try updating your output path from SPT/SG/ERP/Orders/{@this.OrderNumber} to SPT/SG/ERP/Orders/{{this.OrderNumber}} - the {{this.OrderNumber}} syntax will look at the incoming payload for the OrderNumber value(s) and use it dynamically. This is a great overview on this functionality: https://guide.highbyte.com/kb/getting-started/dynamic-references/.","","üëç (3)"
"756543760028139720","aronsemle","2024-06-21T22:51:55.1450000+08:00","In 4.0 (beta out end of next week) we've migrated all flows to pipelines and removed flows for this very reason. It's easier to troubleshoot the pipeline, you can stay in the pipeline UX, and it's faster.","","üëç (4)"
"1248229600853758073","sptha_27377","2024-06-21T23:04:38.9200000+08:00","Hi Guys, Thanks for all your support! I was able to resolve the issue! Entering the reference in the correct format helps a lot. I seem to struggle with HighBytes documentation. Is there anyone who I could DM/email a spec documentation to judge if what I try to do is feasable in HighByte?","",""
"756543760028139720","aronsemle","2024-06-21T23:09:57.2040000+08:00","We try and keep everything here https://guide.highbyte.com/, and have a performance project there that provides some guidance (https://guide.highbyte.com/kb/performance/modeling-opc-ua-data/) but I can DM you with my email contact if you need anything more specific!","","highbyte (1)"
"456226577798135808","Deleted User","2024-06-21T23:25:36.5700000+08:00","Hi Aron!  Thank you for letting us know.  I look forward to work with the 4.0 version. üëç","",""
"817835202746253344","IIoT#4707","2024-06-21T23:25:37.4760000+08:00","GG @Deleted User, you just advanced to level 2!","",""
"1129435706285101076","ted.garrison","2024-06-22T01:29:33.4650000+08:00","Awesome! Can't wait to try it.  I'm just getting started, and yeah, flows vs pipelines is kinda odd.  Will definitely be more simple if it's all just pipelines...","",""
"898217314741280828","hobbes1069","2024-06-22T03:30:40.9170000+08:00","Wow! I knew some changes were coming but that's big!","","4point0 (1),highbyte (1)"
"1214242167640424619","zack.scriven","2024-07-26T08:56:15.4480000+08:00","HighByte... version 4.0... dawg","",""
"1214242167640424619","zack.scriven","2024-07-26T08:56:25.8430000+08:00","Let's do something specials for this....","",""
"1214242167640424619","zack.scriven","2024-07-26T08:56:45.0830000+08:00","i've been waiting for this for years. lol","",""
"1214242167640424619","zack.scriven","2024-08-07T08:00:49.7760000+08:00","especially version 4.20","",""
"1214242167640424619","zack.scriven","2024-08-07T08:00:55.2880000+08:00","needs to be some easter eggs","","ü§£ (1),üö¨ (2)"
"897154180001726494","koios6274","2025-02-28T22:55:58.1800000+08:00","4.1 is out https://www.highbyte.com/resources/release-notes/version-4-1","",""
"219966967480582144","richardphi1618","2025-03-29T20:38:07.2290000+08:00","I am looking to build a data model for wind and solar plants. I work for a company that manages around a dozen or so of these solar plants and plan on growing. Right now the data modeling for each of these sites is based on whatever the manufacturer gives us over opc. I want to do some modeling at the edge within ignition so that upstream systems wont have to worry about this. Is there a standard I should be looking at? I see a couple working groups on opc ua website but Im afraid they might be dead. I am also reading up on OPC UA's online reference and the ISA 95 docs I have access to  but having a tough time seeing if where any of it applies to my problem.","",""
"522568576830537729","codepoet80","2025-03-29T21:45:28.9920000+08:00","There is a working group in progress for exactly this!","",""
"522568576830537729","codepoet80","2025-03-29T21:46:16.1530000+08:00","If you want to PM, I‚Äôll connect you directly with the chair ‚Äî who is a passionate alternative energy entrepreneur and deep expert on UA","",""
"522568576830537729","codepoet80","2025-03-29T21:47:09.8630000+08:00","(And nice to see you again Richard!)","",""
"1073312001788477471","sparkylarks","2025-07-26T21:27:19.4140000+08:00","If you it helps. I've built an integrated data collection system across a number of wind farms and have started calculating power forecast

Waiting for a solar project to kick off now.

Didn't have a standard when we started so had to forge our our way so pulled the manufacturer data, and the substation data into In touch through OPC, Modbus and S7 and pulled out together a reasonably consistent data model an pulled the data back though Kepware to a central Pi historian.

Like all think I'd do it very differently if starting again today","",""
"219966967480582144","richardphi1618","2025-07-26T22:22:20.9860000+08:00","I will say it is more critical with solar sites because there are way more devices and by extension data than say a similarly sized wind site of similar or equal power","","üëç (1)"
"219966967480582144","richardphi1618","2025-07-26T22:23:59.6310000+08:00","I am currently working with cirrus link on this as our main aggregation tooling and dropping the data into snowflake for warehousing and transformations I feel like I have the right tools... I just need to do it better :blobderpy:","",""
"812295088348200960","patanj2","2025-07-28T22:50:36.6110000+08:00","Q","",""
