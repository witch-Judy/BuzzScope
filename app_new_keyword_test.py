"""
Simple Historical Analysis App
ç®€åŒ–ç‰ˆå†å²æ•°æ®åˆ†æåº”ç”¨ï¼Œé¿å…å¤æ‚å¯¼å…¥é—®é¢˜
"""

import streamlit as st
import streamlit.components.v1 as components
import json
import os
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional, Tuple

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="BuzzScope - New Keyword Test",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

class SimpleHistoricalAnalyzer:
    """ç®€åŒ–çš„å†å²æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = cache_dir
        self.platforms = ["hackernews", "reddit", "youtube"]  # Discord removed for new keyword analysis
        self.default_keywords = ["ai", "iot", "mqtt", "unified_namespace"]
        
        # Hacker News parquetåˆ†æå™¨å·²ç§»é™¤ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®
        
        # å¯¼å…¥æ”¶é›†å™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…Streamlitå¯åŠ¨æ—¶çš„ä¾èµ–é—®é¢˜ï¼‰
        self._collectors = None
    
    
    def _get_collectors(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ”¶é›†å™¨"""
        if self._collectors is None:
            try:
                # åŠ¨æ€å¯¼å…¥æ”¶é›†å™¨
                from src.collectors.reddit_collector import RedditCollector
                from src.collectors.youtube_collector import YouTubeCollector
                from src.analyzers.hackernews_parquet_analyzer import HackerNewsParquetAnalyzer
                
                self._collectors = {
                    "hackernews": HackerNewsParquetAnalyzer(),  # ä½¿ç”¨parquetæ•°æ®è€Œä¸æ˜¯å®æ—¶API
                    "reddit": RedditCollector(),
                    "youtube": YouTubeCollector()
                    # æ³¨æ„ï¼šä¸åŒ…å«Discordï¼Œå› ä¸ºDiscordåªæœ‰å†å²æ•°æ®
                }
            except ImportError as e:
                st.error(f"Failed to import collectors: {e}")
                self._collectors = {}
        return self._collectors
    
    def collect_real_data(self, keyword: str) -> Dict[str, Any]:
        """æ”¶é›†æ–°å…³é”®è¯çš„çœŸå®æ•°æ®"""
        collectors = self._get_collectors()
        
        if not collectors:
            return {
                "status": "error",
                "message": "Failed to initialize data collectors"
            }
        
        results = {
            "keyword": keyword,
            "status": "collecting",
            "platforms": {},
            "start_time": datetime.now().isoformat()
        }
        
        # åªæ”¶é›†æœ‰APIçš„å¹³å°æ•°æ®ï¼ˆæ’é™¤Discordï¼‰
        platforms_to_collect = ["hackernews", "reddit", "youtube"]
        
        for platform in platforms_to_collect:
            try:
                st.info(f"ğŸ” Collecting {platform} data for '{keyword}'...")
                
                collector = collectors[platform]
                
                if platform == "hackernews":
                    # Hacker Newsä½¿ç”¨parquetæ•°æ®ï¼Œè°ƒç”¨calculate_metricsæ–¹æ³•
                    with st.spinner(f"Searching Hacker News parquet data for '{keyword}'... This may take 1-2 minutes"):
                        metrics = collector.calculate_metrics(keyword, exact_match=True)
                    
                    if not metrics or metrics.get('total_posts', 0) == 0:
                        st.warning(f"âš ï¸ No posts found for '{keyword}' on {platform}")
                        results["platforms"][platform] = {
                            "status": "no_data",
                            "posts_collected": 0,
                            "message": f"No posts found for '{keyword}' on {platform}"
                        }
                        continue
                    
                    # ä¿å­˜Hacker Newsæ•°æ®åˆ°ç¼“å­˜
                    try:
                        self._save_hackernews_to_cache(platform, keyword, metrics)
                        
                        results["platforms"][platform] = {
                            "status": "success",
                            "posts_collected": metrics.get('total_posts', 0),
                            "cache_file": f"{keyword}.json",
                            "message": f"Successfully collected {metrics.get('total_posts', 0)} posts from parquet data"
                        }
                        
                        st.success(f"âœ… Collected {metrics.get('total_posts', 0)} posts from {platform} parquet data")
                        
                    except Exception as save_error:
                        st.error(f"âŒ Error saving {platform} data: {save_error}")
                        results["platforms"][platform] = {
                            "status": "save_error",
                            "error": str(save_error),
                            "posts_collected": metrics.get('total_posts', 0) if metrics else 0
                        }
                        
                else:
                    # Redditå’ŒYouTubeä½¿ç”¨å®æ—¶API
                    with st.spinner(f"Searching {platform} for '{keyword}'... This may take 1-3 minutes"):
                        if platform == "reddit":
                            # Redditä½¿ç”¨å…¨å±€æœç´¢ï¼Œæ—¶é—´èŒƒå›´è®¾ä¸ºæ‰€æœ‰æ—¶é—´
                            posts = collector.search_keyword(keyword, days_back=365*5, exact_match=True, use_global=True)
                        else:
                            # YouTubeä½¿ç”¨æ‰€æœ‰æ—¶é—´æœç´¢
                            posts = collector.search_keyword(keyword, days_back=365*5, exact_match=True)
                    
                    if not posts:
                        st.warning(f"âš ï¸ No posts found for '{keyword}' on {platform}")
                        results["platforms"][platform] = {
                            "status": "no_data",
                            "posts_collected": 0,
                            "message": f"No posts found for '{keyword}' on {platform}"
                        }
                        continue
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    try:
                        self._save_real_data_to_cache(platform, keyword, posts)
                        
                        results["platforms"][platform] = {
                            "status": "success",
                            "posts_collected": len(posts),
                            "cache_file": f"{keyword}.json"
                        }
                        
                        st.success(f"âœ… Collected {len(posts)} posts from {platform}")
                        
                    except Exception as save_error:
                        st.error(f"âŒ Error saving {platform} data: {save_error}")
                        results["platforms"][platform] = {
                            "status": "save_error",
                            "error": str(save_error),
                            "posts_collected": len(posts) if posts else 0
                        }
                
            except Exception as e:
                st.error(f"âŒ Error collecting data for {platform}: {e}")
                results["platforms"][platform] = {
                    "status": "error",
                    "error": str(e)
                }
        
        results["end_time"] = datetime.now().isoformat()
        results["status"] = "completed"
        
        return results
    
    def _save_real_data_to_cache(self, platform: str, keyword: str, posts: List[Any]):
        """ä¿å­˜çœŸå®æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶"""
        cache_dir = os.path.join(self.cache_dir, platform)
        os.makedirs(cache_dir, exist_ok=True)
        
        cache_file = os.path.join(cache_dir, f"{keyword}.json")
        
        # å°†å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¦‚æœæ˜¯å¯¹è±¡çš„è¯ï¼‰
        serializable_posts = []
        for post in posts:
            if hasattr(post, '__dict__'):
                # å¦‚æœæ˜¯å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                post_dict = post.__dict__.copy()
                # å¤„ç†datetimeå¯¹è±¡
                for key, value in post_dict.items():
                    if hasattr(value, 'isoformat'):
                        post_dict[key] = value.isoformat()
                serializable_posts.append(post_dict)
            else:
                # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                serializable_posts.append(post)
        
        # åˆ›å»ºç¼“å­˜æ•°æ®ç»“æ„
        cache_data = {
            "keyword": keyword,
            "platform": platform,
            "data_source": "time_all" if platform in ["reddit", "youtube"] else "cache",
            "collection_time": datetime.now().isoformat(),
            "posts": serializable_posts,
            "total_posts": len(serializable_posts)
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        st.success(f"ğŸ’¾ Cached {len(serializable_posts)} posts to {cache_file}")
        
        # ä¸ºæ–°æ•°æ®ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨
        self._generate_trend_chart(platform, keyword, serializable_posts)
    
    def _save_hackernews_to_cache(self, platform: str, keyword: str, metrics: Dict[str, Any]) -> None:
        """ä¿å­˜Hacker News parquetæ•°æ®åˆ°ç¼“å­˜"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            platform_dir = os.path.join(self.cache_dir, platform)
            os.makedirs(platform_dir, exist_ok=True)
            
            # åˆ›å»ºç¼“å­˜æ•°æ®ç»“æ„
            cache_data = {
                "keyword": keyword,
                "platform": platform,
                "data_source": "parquet",
                "collected_at": datetime.now().isoformat(),
                "metrics": metrics,
                "total_posts": metrics.get('total_posts', 0)
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            file_path = os.path.join(platform_dir, f"{keyword}.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            st.info(f"ğŸ’¾ Saved Hacker News metrics to cache: {file_path}")
            
        except Exception as e:
            st.error(f"âŒ Error saving Hacker News to cache: {e}")
            raise
    
    def _generate_trend_chart(self, platform: str, keyword: str, posts: List[Dict[str, Any]]):
        """ä¸ºæ–°æ”¶é›†çš„æ•°æ®ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨"""
        try:
            # è®¡ç®—æœˆåº¦æ•°æ®
            monthly_mentions = self._calculate_monthly_mentions(posts)
            
            if not monthly_mentions:
                return
            
            # åˆ›å»ºå›¾è¡¨
            months = sorted(monthly_mentions.keys())
            counts = [monthly_mentions[month] for month in months]
            
            fig = go.Figure(data=go.Scatter(
                x=months,
                y=counts,
                mode='lines+markers',
                name='Monthly Mentions',
                line=dict(color='#1f77b4', width=3),
                marker=dict(size=8, color='#1f77b4')
            ))
            
            fig.update_layout(
                title=f"Monthly Mentions Trend for {platform.title()}",
                xaxis_title="Month",
                yaxis_title="Number of Posts",
                height=400,
                showlegend=False
            )
            
            # ç”ŸæˆHTML
            chart_html = fig.to_html(include_plotlyjs=False, config={'displayModeBar': False})
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_months = len(monthly_mentions)
            total_posts = sum(monthly_mentions.values())
            avg_posts = total_posts / total_months if total_months > 0 else 0
            most_active_month = max(monthly_mentions.items(), key=lambda x: x[1])[0] if monthly_mentions else None
            
            # ä¿å­˜å›¾è¡¨ç¼“å­˜
            chart_cache = {
                "chart_html": chart_html,
                "statistics": {
                    "most_active_month": most_active_month,
                    "average_posts_per_month": f"{avg_posts:.1f}",
                    "total_months": total_months,
                    "total_posts": total_posts
                }
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜ç›®å½•
            charts_dir = os.path.join(self.cache_dir, "charts")
            os.makedirs(charts_dir, exist_ok=True)
            
            chart_file = os.path.join(charts_dir, f"{platform}_{keyword}.json")
            with open(chart_file, 'w', encoding='utf-8') as f:
                json.dump(chart_cache, f, ensure_ascii=False, indent=2)
            
            st.success(f"ğŸ“Š Generated trend chart for {platform}")
            
        except Exception as e:
            st.warning(f"âš ï¸ Could not generate trend chart for {platform}: {e}")
    
    def load_platform_data(self, platform: str, keyword: str) -> Dict[str, Any]:
        """åŠ è½½å¹³å°æ•°æ®"""
        file_path = os.path.join(self.cache_dir, platform, f"{keyword}.json")
        if not os.path.exists(file_path):
            return {"status": "no_data", "posts": []}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            st.error(f"Error loading {file_path}: {e}")
            return {"status": "error", "posts": []}
    
    def calculate_platform_metrics(self, platform: str, keyword: str) -> Dict[str, Any]:
        """è®¡ç®—å•ä¸ªå¹³å°çš„æŒ‡æ ‡"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ•°æ®
        data = self.load_platform_data(platform, keyword)
        
        # å¦‚æœç¼“å­˜æ•°æ®å­˜åœ¨ä¸”åŒ…å«metricsï¼Œç›´æ¥è¿”å›
        if data.get("metrics"):
            return data["metrics"]
        
        # å¦‚æœç¼“å­˜æ•°æ®å­˜åœ¨ä½†æ²¡æœ‰metricsï¼Œä»postsè®¡ç®—
        posts = data.get("posts", [])
        
        if not posts:
            return {
                "platform": platform,
                "keyword": keyword,
                "total_posts": 0,
                "total_interactions": 0,
                "unique_authors": 0,
                "top_contributors": [],
                "top_posts": [],
                "monthly_mentions": {}
            }
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        total_posts = len(posts)
        total_interactions = sum(self._get_interaction_count(post) for post in posts)
        unique_authors = len(set(self._get_author(post) for post in posts if self._get_author(post)))
        
        # è®¡ç®—Topè´¡çŒ®è€…
        author_counts = Counter(self._get_author(post) for post in posts if self._get_author(post))
        top_contributors = [
            {
                "author": author,
                "post_count": count,
                "platform": platform,
                "profile_url": self._get_author_url(platform, author)
            }
            for author, count in author_counts.most_common(10)
        ]
        
        # è®¡ç®—Topå¸–å­
        top_posts = sorted(posts, key=lambda x: self._get_interaction_count(x), reverse=True)[:10]
        top_posts = [
            {
                "title": post.get('title', 'No title'),
                "interactions": self._get_interaction_count(post),
                "author": self._get_author(post),
                "created_at": post.get('created_at', ''),
                "url": self._get_post_url(platform, post),
                "platform": platform
            }
            for post in top_posts
        ]
        
        # è®¡ç®—æœˆåº¦è¶‹åŠ¿æ•°æ®
        monthly_mentions = self._calculate_monthly_mentions(posts)
        
        return {
            "platform": platform,
            "keyword": keyword,
            "total_posts": total_posts,
            "total_interactions": total_interactions,
            "unique_authors": unique_authors,
            "top_contributors": top_contributors,
            "top_posts": top_posts,
            "monthly_mentions": monthly_mentions
        }
    
    def _get_interaction_count(self, post: Dict[str, Any]) -> int:
        """è·å–å¸–å­çš„äº’åŠ¨æ•°"""
        platform = post.get('platform', '')
        
        if platform == 'reddit':
            return post.get('score', 0) + post.get('num_comments', 0)
        elif platform == 'youtube':
            # YouTubeäº’åŠ¨æ•°è®¡ç®—ï¼šè§‚çœ‹æ•°/100 + ç‚¹èµæ•° + è¯„è®ºæ•°
            # è¿™æ ·æ—¢è€ƒè™‘äº†è§‚çœ‹æ•°ï¼Œåˆä¸ä¼šè®©è§‚çœ‹æ•°å®Œå…¨ä¸»å¯¼æ’å
            view_count = (post.get('view_count', 0) or 0)
            like_count = (post.get('like_count', 0) or 0)
            comment_count = (post.get('comment_count', 0) or 0)
            return (view_count // 100) + like_count + comment_count
        elif platform == 'hackernews':
            return post.get('score', 0) + post.get('descendants', 0)
        elif platform == 'discord':
            # Discordäº’åŠ¨æ•°è®¡ç®—ï¼šå¦‚æœæœ‰reactionsæ•°æ®å°±ä½¿ç”¨ï¼Œå¦åˆ™é»˜è®¤ä¸º1ï¼ˆè¡¨ç¤ºæœ‰å†…å®¹ï¼‰
            reactions = post.get('reactions', 0)
            if reactions == 0 or reactions == "No reactions":
                return 1  # è‡³å°‘è¡¨ç¤ºæœ‰å†…å®¹è¢«å‘å¸ƒ
            return int(reactions) if isinstance(reactions, (int, str)) and str(reactions).isdigit() else 1
        else:
            return 0
    
    def _get_author(self, post: Dict[str, Any]) -> Optional[str]:
        """è·å–å¸–å­ä½œè€…"""
        platform = post.get('platform', '')
        
        if platform == 'reddit':
            return post.get('author', '')
        elif platform == 'youtube':
            return post.get('author', '') or post.get('channel_title', '')
        elif platform == 'hackernews':
            return post.get('by', '')
        elif platform == 'discord':
            return post.get('author', '')
        else:
            return None
    
    def _get_author_url(self, platform: str, author: str) -> str:
        """è·å–ä½œè€…é“¾æ¥"""
        if platform == 'reddit':
            return f"https://reddit.com/u/{author}"
        elif platform == 'youtube':
            return f"https://youtube.com/@{author.replace(' ', '')}"
        elif platform == 'hackernews':
            return f"https://news.ycombinator.com/user?id={author}"
        elif platform == 'discord':
            return f"https://discord.com/users/{author}"
        else:
            return ""
    
    def _get_post_url(self, platform: str, post: Dict[str, Any]) -> str:
        """è·å–å¸–å­é“¾æ¥"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ä¸­å·²å­˜å‚¨çš„URL
        if post.get('url'):
            return post.get('url')
        
        # å¦‚æœæ²¡æœ‰URLï¼Œåˆ™æ„å»ºURL
        if platform == 'reddit':
            return f"https://reddit.com{post.get('permalink', '')}"
        elif platform == 'youtube':
            return f"https://youtube.com/watch?v={post.get('video_id', '')}"
        elif platform == 'hackernews':
            return f"https://news.ycombinator.com/item?id={post.get('id', '')}"
        elif platform == 'discord':
            return post.get('jump_url', '')
        else:
            return ""
    
    def _calculate_monthly_mentions(self, posts: List[Dict[str, Any]]) -> Dict[str, int]:
        """è®¡ç®—æ¯æœˆæåŠæ•°"""
        monthly_counts = defaultdict(int)
        
        for post in posts:
            # å°è¯•å¤šä¸ªæ—¶é—´æˆ³å­—æ®µ
            timestamp_fields = ['created_at', 'timestamp', 'published_at', 'upload_date']
            date_obj = None
            
            for field in timestamp_fields:
                timestamp = post.get(field, '')
                if timestamp:
                    try:
                        # å¤„ç†ä¸åŒçš„æ—¶é—´æˆ³æ ¼å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                        if ' ' in timestamp and 'UTC' in timestamp:
                            # æ ¼å¼: 2025-10-04 09:02:33 UTC (æœ€é«˜ä¼˜å…ˆçº§)
                            date_obj = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S UTC')
                        elif 'T' in timestamp:
                            # ISOæ ¼å¼: 2022-10-18T17:00:10+00:00
                            date_obj = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        elif ' ' in timestamp and '+' in timestamp:
                            # æ ¼å¼: 2022-10-18 17:00:10+00:00
                            date_obj = datetime.fromisoformat(timestamp)
                        elif ' ' in timestamp:
                            # æ ¼å¼: 2022-10-18 17:00:10
                            date_obj = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
                        elif timestamp.isdigit() and len(timestamp) > 10:
                            # Discord snowflake IDæ ¼å¼: é•¿æ•°å­—å­—ç¬¦ä¸²
                            # Discord snowflake IDåŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œéœ€è¦è½¬æ¢
                            snowflake_id = int(timestamp)
                            # Discord snowflake IDçš„æ—¶é—´æˆ³æ˜¯æ¯«ç§’ï¼Œéœ€è¦é™¤ä»¥1000
                            # å¹¶ä¸”éœ€è¦åŠ ä¸ŠDiscordçš„epochæ—¶é—´ (2015-01-01)
                            discord_epoch = 1420070400000  # 2015-01-01 00:00:00 UTC in milliseconds
                            timestamp_ms = (snowflake_id >> 22) + discord_epoch
                            date_obj = datetime.fromtimestamp(timestamp_ms / 1000)
                        else:
                            # å°è¯•å…¶ä»–æ ¼å¼
                            date_obj = datetime.fromisoformat(timestamp)
                        break
                    except Exception as e:
                        print(f"Error parsing timestamp '{timestamp}': {e}")
                        continue
            
            if date_obj:
                month_str = date_obj.strftime('%Y-%m')
                monthly_counts[month_str] += 1
        
        return dict(monthly_counts)

def display_platform_overview(metrics: Dict[str, Any]):
    """æ˜¾ç¤ºå¹³å°æ¦‚è§ˆ"""
    st.subheader("Platform Overview")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="Total Posts",
            value=metrics.get('total_posts', 0),
            help="è¯¥å¹³å°çš„æ€»å¸–å­æ•°"
        )
    
    with col2:
        st.metric(
            label="Total Interactions", 
            value=metrics.get('total_interactions', 0),
            help="è¯¥å¹³å°çš„æ€»äº’åŠ¨æ•°"
        )
    
    with col3:
        st.metric(
            label="Unique Authors",
            value=metrics.get('unique_authors', 0),
            help="è¯¥å¹³å°çš„ç‹¬ç‰¹ä½œè€…æ•°"
        )

def display_top_contributors(contributors: List[Dict[str, Any]], platform: str):
    """æ˜¾ç¤ºTopè´¡çŒ®è€…"""
    if not contributors:
        st.info("No contributors found")
        return
    
    st.subheader(f"Top Contributors - {platform.title()}")
    
    for i, contributor in enumerate(contributors[:10]):
        col1, col2, col3 = st.columns([3, 1, 1])
        
        with col1:
            if contributor.get('profile_url'):
                st.markdown(f"[{contributor['author']}]({contributor['profile_url']})")
            else:
                st.write(contributor['author'])
        
        with col2:
            st.write(f"{contributor['post_count']} posts")
        
        with col3:
            st.write(platform.title())

# Discord special display function removed - not needed for new keyword analysis

def display_top_posts(posts: List[Dict[str, Any]], platform: str):
    """æ˜¾ç¤ºTopå¸–å­"""
    if not posts:
        st.info("No posts found")
        return
    
    st.subheader(f"Top Posts - {platform.title()}")
    
    for i, post in enumerate(posts[:10]):
        with st.expander(f"#{i+1} {post['title'][:50]}..."):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.write(f"**Title:** {post['title']}")
                st.write(f"**Author:** {post['author']}")
                st.write(f"**Date:** {post['created_at']}")
            
            with col2:
                st.write(f"**Interactions:** {post['interactions']}")
                if post.get('url'):
                    st.markdown(f"[View Post]({post['url']})")

def load_cached_chart(platform: str, keyword: str) -> Dict[str, Any]:
    """åŠ è½½ç¼“å­˜çš„å›¾è¡¨æ•°æ®"""
    # å°è¯•ä¸¤ç§æ–‡ä»¶åæ ¼å¼
    chart_files = [
        f"data/cache/charts/{platform}_{keyword}.json",
        f"data/cache/charts/{platform}_{keyword}_trend.json"
    ]
    
    for chart_file in chart_files:
        if os.path.exists(chart_file):
            try:
                with open(chart_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Error loading chart {chart_file}: {e}")
    
    return None

def display_trend_analysis(metrics: Dict[str, Any], platform: str, keyword: str):
    """æ˜¾ç¤ºæœˆåº¦è¶‹åŠ¿åˆ†æ"""
    st.subheader(f"Monthly Trend Analysis - {platform.title()}")
    
    # é¦–å…ˆæ˜¾ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼
    monthly_mentions = metrics.get('monthly_mentions', {})
    if monthly_mentions:
        st.markdown("**Raw Monthly Data**")
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        months = sorted(monthly_mentions.keys())
        table_data = []
        for month in months:
            table_data.append({
                "Month": month,
                "Posts": monthly_mentions[month]
            })
        
        # æ˜¾ç¤ºè¡¨æ ¼
        df = pd.DataFrame(table_data)
        st.dataframe(df, width='stretch')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_posts = sum(monthly_mentions.values())
        avg_posts = total_posts / len(months) if months else 0
        most_active_month = max(monthly_mentions.items(), key=lambda x: x[1])[0] if monthly_mentions else None
        
        st.markdown("**Monthly Statistics**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Posts", total_posts)
        with col2:
            st.metric("Average/Month", f"{avg_posts:.1f}")
        with col3:
            st.metric("Most Active Month", most_active_month or 'N/A')
        
        st.markdown("**Trend Chart**")
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„å›¾è¡¨
    cached_chart = load_cached_chart(platform, keyword)
    
    if cached_chart:
        # å°è¯•ä»ç¼“å­˜çš„HTMLä¸­æå–Plotlyæ•°æ®å¹¶é‡æ–°åˆ›å»ºå›¾è¡¨
        try:
            # ä»HTMLä¸­æå–æ•°æ®å¹¶é‡æ–°åˆ›å»ºPlotlyå›¾è¡¨
            monthly_mentions = metrics.get('monthly_mentions', {})
            if monthly_mentions:
                months = sorted(monthly_mentions.keys())
                counts = [monthly_mentions[month] for month in months]
                
                fig = go.Figure(data=go.Scatter(
                    x=months,
                    y=counts,
                    mode='lines+markers',
                    name='Monthly Mentions',
                    line=dict(color='#1f77b4', width=3),
                    marker=dict(size=8, color='#1f77b4')
                ))
                
                fig.update_layout(
                    title=f"Monthly Mentions Trend for {platform.title()}",
                    xaxis_title="Month",
                    yaxis_title="Number of Posts",
                    height=400,
                    showlegend=False
                )
                
                st.plotly_chart(fig, width='stretch', config={'displayModeBar': False})
            else:
                # å›é€€åˆ°HTMLæ˜¾ç¤º
                chart_html = cached_chart['chart_html']
                if 'plotly.js' not in chart_html.lower():
                    plotly_js = '<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>'
                    chart_html = chart_html.replace('<head>', f'<head>{plotly_js}')
                st.components.v1.html(chart_html, height=450)
        except Exception as e:
            st.error(f"Error displaying chart: {e}")
            # å›é€€åˆ°HTMLæ˜¾ç¤º
            chart_html = cached_chart['chart_html']
            if 'plotly.js' not in chart_html.lower():
                plotly_js = '<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>'
                chart_html = chart_html.replace('<head>', f'<head>{plotly_js}')
            st.components.v1.html(chart_html, height=450)
        
        # æ˜¾ç¤ºç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯
        stats = cached_chart.get('statistics', {})
        st.markdown("**Monthly Statistics**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if stats.get('most_active_month'):
                st.metric("Most Active Month", stats['most_active_month'])
        
        with col2:
            if stats.get('average_posts_per_month'):
                st.metric("Average Posts/Month", f"{stats['average_posts_per_month']}")
    else:
        # å›é€€åˆ°å®æ—¶è®¡ç®—
        monthly_mentions = metrics.get('monthly_mentions', {})
        
        if not monthly_mentions:
            st.info("No monthly trend data available")
            return
        
        # åˆ›å»ºæœˆåº¦è¶‹åŠ¿å›¾
        months = sorted(monthly_mentions.keys())
        counts = [monthly_mentions[month] for month in months]
        
        # ç¡®ä¿æœ‰æ•°æ®æ‰åˆ›å»ºå›¾è¡¨
        if not months or not counts:
            st.info("No monthly trend data available")
            return
        
        fig = go.Figure(data=go.Scatter(
            x=months,
            y=counts,
            mode='lines+markers',
            name='Monthly Mentions',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8, color='#1f77b4')
        ))
        
        fig.update_layout(
            title=f"Monthly Mentions Trend for {platform.title()}",
            xaxis_title="Month",
            yaxis_title="Number of Posts",
            height=400,
            showlegend=False
        )
        
        st.plotly_chart(fig, width='stretch', config={'displayModeBar': False})
        
        # æ˜¾ç¤ºæœˆåº¦ç»Ÿè®¡ä¿¡æ¯
        st.write("### Monthly Statistics")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if monthly_mentions:
                max_month = max(monthly_mentions.items(), key=lambda x: x[1])
                st.metric("Most Active Month", f"{max_month[0]} ({max_month[1]} posts)")
        
        with col2:
            if monthly_mentions:
                total_months = len(monthly_mentions)
                total_posts = sum(monthly_mentions.values())
                avg_posts = total_posts / total_months if total_months > 0 else 0
                st.metric("Average Posts/Month", f"{avg_posts:.1f}")

def main():
    st.title("BuzzScope: Where technology speaks, and you feel the echo")
    st.markdown("---")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SimpleHistoricalAnalyzer()
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("Analysis Configuration")
    
    # å…³é”®è¯é€‰æ‹©
    st.sidebar.subheader("Keywords")
    
    # ç»Ÿä¸€çš„å…³é”®è¯è¾“å…¥
    keyword_input = st.sidebar.text_input(
        "Enter keywords (comma-separated):",
        value="ai, iot, mqtt, unified_namespace",
        placeholder="e.g., ai, blockchain, python, machine learning",
        help="Enter keywords separated by commas. Default keywords are pre-filled."
    )
    
    # è§£æå…³é”®è¯
    if keyword_input:
        selected_keywords = [kw.strip() for kw in keyword_input.split(',') if kw.strip()]
    else:
        selected_keywords = []
    
    # æ•°æ®æ”¶é›†æŒ‰é’®
    collect_data = False
    if selected_keywords:
        st.sidebar.markdown("---")
        st.sidebar.markdown("**Data Status**")
        
        # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯çš„æ•°æ®çŠ¶æ€
        keywords_to_collect = []
        for keyword in selected_keywords:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜æ•°æ®
            has_cached_data = any(
                analyzer.load_platform_data(platform, keyword).get("posts") or 
                analyzer.load_platform_data(platform, keyword).get("metrics")
                for platform in ["hackernews", "reddit", "youtube"]
            )
            
            if has_cached_data:
                st.sidebar.success(f"âœ… '{keyword}' - cached")
            else:
                st.sidebar.warning(f"âš ï¸ '{keyword}' - needs collection")
                keywords_to_collect.append(keyword)
        
        # å¦‚æœæœ‰éœ€è¦æ”¶é›†çš„å…³é”®è¯ï¼Œæ˜¾ç¤ºæ”¶é›†æŒ‰é’®
        if keywords_to_collect:
            st.sidebar.info(f"Need to collect data for: {', '.join(keywords_to_collect)}")
            if st.sidebar.button("ğŸš€ Collect Missing Data", type="primary"):
                collect_data = True
    
    # å¹³å°é€‰æ‹©
    selected_platforms = st.sidebar.multiselect(
        "Select Platforms:",
        analyzer.platforms,
        default=analyzer.platforms
    )
    
    # åˆ†æç±»å‹é€‰æ‹©
    analysis_type = st.sidebar.selectbox(
        "Analysis Type:",
        ["Single Keyword Analysis", "Cross-Platform Comparison"]
    )
    
    if not selected_keywords:
        st.warning("Please select at least one keyword")
        return
    
    if not selected_platforms:
        st.warning("Please select at least one platform")
        return
    
    # å¤„ç†æ•°æ®æ”¶é›†
    if collect_data and keywords_to_collect:
        st.markdown("---")
        st.subheader(f"ğŸš€ Collecting Data for: {', '.join(keywords_to_collect)}")
        st.info("â±ï¸ Data collection may take 2-7 minutes per keyword. Please be patient...")
        
        all_success = True
        for keyword in keywords_to_collect:
            st.write(f"ğŸ“Š Collecting data for '{keyword}'...")
            results = analyzer.collect_real_data(keyword)
            
            if results["status"] == "completed":
                st.success(f"âœ… Data collection completed for '{keyword}'!")
            else:
                st.error(f"âŒ Data collection failed for '{keyword}'")
                all_success = False
        
        if all_success:
            st.success("ğŸ‰ All data collection completed!")
            st.info("ğŸ”„ Refreshing page to show new data...")
            st.rerun()
        else:
            st.error("âŒ Some data collection failed. Please check the error messages above.")
    
    # ä¸»å†…å®¹åŒºåŸŸ
    if analysis_type == "Single Keyword Analysis":
        display_single_keyword_analysis(analyzer, selected_keywords, selected_platforms)
    elif analysis_type == "Cross-Platform Comparison":
        display_cross_platform_comparison(analyzer, selected_keywords, selected_platforms)

def display_single_keyword_analysis(analyzer, keywords, platforms):
    """æ˜¾ç¤ºå•å…³é”®è¯åˆ†æ"""
    # å¦‚æœåªæœ‰ä¸€ä¸ªå…³é”®è¯ï¼Œç›´æ¥åˆ†æ
    if len(keywords) == 1:
        selected_keyword = keywords[0]
    else:
        # å¦‚æœæœ‰å¤šä¸ªå…³é”®è¯ï¼Œè®©ç”¨æˆ·é€‰æ‹©ä¸€ä¸ª
        selected_keyword = st.selectbox(
            "Select a keyword to analyze:",
            keywords,
            help="Choose one keyword from the list to perform detailed analysis"
        )
        if not selected_keyword:
            return
    
    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    with st.spinner(f"Analyzing {selected_keyword}..."):
        # åˆ†æå…³é”®è¯
        results = {}
        for platform in platforms:
            results[platform] = analyzer.calculate_platform_metrics(platform, selected_keyword)
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.subheader(f"Overall Statistics for '{selected_keyword}'")
    
    total_posts = sum(metrics['total_posts'] for metrics in results.values())
    total_interactions = sum(metrics['total_interactions'] for metrics in results.values())
    # ç»Ÿè®¡å„å¹³å°ä½œè€…æ€»æ•°ï¼ˆå¯èƒ½åŒ…å«è·¨å¹³å°é‡å¤ï¼‰
    total_authors = sum(metrics['unique_authors'] for metrics in results.values())
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Posts", total_posts)
    with col2:
        st.metric("Total Interactions", total_interactions)
    with col3:
        st.metric("Unique Authors", total_authors)
    
    # æ˜¾ç¤ºå„å¹³å°åˆ†æ
    for i, (platform, metrics) in enumerate(results.items()):
        if metrics['total_posts'] > 0:
            # å¹³å°åˆ†éš”çº¿ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªå¹³å°ï¼‰
            if i > 0:
                st.markdown("---")
            
            # å¹³å°æ ‡é¢˜
            st.header(f"{platform.title()} Analysis")
            
            # å¹³å°å¸¸è§„å¤„ç†
            # å¹³å°æ¦‚è§ˆ
            display_platform_overview(metrics)
            
            # Topè´¡çŒ®è€…
            display_top_contributors(metrics['top_contributors'], platform)
            
            # Topå¸–å­
            display_top_posts(metrics['top_posts'], platform)
            
            # è¶‹åŠ¿åˆ†æ
            display_trend_analysis(metrics, platform, selected_keyword)
            
        else:
            # æ— æ•°æ®æ—¶çš„åˆ†éš”çº¿
            if i > 0:
                st.markdown("---")
            st.info(f"{platform.title()}: No data available")

def display_cross_platform_comparison(analyzer, keywords, platforms):
    """æ˜¾ç¤ºè·¨å¹³å°å¯¹æ¯”åˆ†æ"""
    st.header("Cross-Platform Comparison")
    
    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    with st.spinner("Analyzing keywords..."):
        # æ”¶é›†æ‰€æœ‰å…³é”®è¯çš„æ•°æ®
        all_results = {}
        for keyword in keywords:
            all_results[keyword] = {}
            for platform in platforms:
                all_results[keyword][platform] = analyzer.calculate_platform_metrics(platform, keyword)
    
    # åˆ›å»ºå¯¹æ¯”æ•°æ®
    comparison_data = []
    for keyword in keywords:
        for platform in platforms:
            metrics = all_results[keyword][platform]
            comparison_data.append({
                'keyword': keyword,
                'platform': platform,
                'posts': metrics['total_posts'],
                'interactions': metrics['total_interactions'],
                'authors': metrics['unique_authors']
            })
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig = px.bar(
            df, 
            x='platform', 
            y='posts', 
            color='keyword',
            title='Cross-Platform Comparison: Posts by Platform and Keyword',
            barmode='group'
        )
        
        st.plotly_chart(fig, width='stretch', config={'displayModeBar': False})
        
        # æ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        st.write("### Detailed Comparison")
        st.dataframe(df, width='stretch')

if __name__ == "__main__":
    main()
