"""
ç®€å•æµ‹è¯•å†å²æ•°æ®åˆ†æåŠŸèƒ½
"""

import json
import os
from datetime import datetime
from collections import Counter

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
    
    cache_dir = "data/cache"
    platforms = ["reddit", "youtube", "discord"]
    keywords = ["ai", "iot", "mqtt", "unified_namespace"]
    
    total_posts = 0
    for platform in platforms:
        platform_posts = 0
        for keyword in keywords:
            file_path = os.path.join(cache_dir, platform, f"{keyword}.json")
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        posts = data.get('total_posts', 0)
                        platform_posts += posts
                        total_posts += posts
                        print(f"  {platform}/{keyword}: {posts} posts")
                except Exception as e:
                    print(f"  Error loading {file_path}: {e}")
        
        print(f"  {platform} total: {platform_posts} posts")
    
    print(f"\nğŸ“Š æ€»è®¡: {total_posts} posts")
    return total_posts > 0

def test_metrics_calculation():
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
    print("\nğŸ“ˆ æµ‹è¯•æŒ‡æ ‡è®¡ç®—...")
    
    # åŠ è½½ä¸€ä¸ªæ ·æœ¬æ–‡ä»¶
    file_path = "data/cache/reddit/ai.json"
    if not os.path.exists(file_path):
        print("  âŒ æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        posts = data.get('posts', [])
        print(f"  ğŸ“Š åŠ è½½äº† {len(posts)} ä¸ªå¸–å­")
        
        if posts:
            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            total_posts = len(posts)
            total_interactions = 0
            authors = []
            
            for post in posts:
                # è®¡ç®—äº’åŠ¨æ•°
                score = post.get('score', 0)
                comments = post.get('num_comments', 0)
                total_interactions += score + comments
                
                # æ”¶é›†ä½œè€…
                author = post.get('author', '')
                if author:
                    authors.append(author)
            
            unique_authors = len(set(authors))
            
            print(f"    - æ€»å¸–å­æ•°: {total_posts}")
            print(f"    - æ€»äº’åŠ¨æ•°: {total_interactions}")
            print(f"    - ç‹¬ç‰¹ä½œè€…æ•°: {unique_authors}")
            
            # è®¡ç®—Topè´¡çŒ®è€…
            author_counts = Counter(authors)
            top_contributors = author_counts.most_common(5)
            print(f"    - Topè´¡çŒ®è€…: {top_contributors}")
            
            return True
        else:
            print("  âŒ æ²¡æœ‰å¸–å­æ•°æ®")
            return False
            
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        return False

def test_trend_analysis():
    """æµ‹è¯•è¶‹åŠ¿åˆ†æ"""
    print("\nğŸ“ˆ æµ‹è¯•è¶‹åŠ¿åˆ†æ...")
    
    file_path = "data/cache/reddit/ai.json"
    if not os.path.exists(file_path):
        print("  âŒ æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        posts = data.get('posts', [])
        if not posts:
            print("  âŒ æ²¡æœ‰å¸–å­æ•°æ®")
            return False
        
        # è®¡ç®—æ¯æ—¥æåŠ
        daily_mentions = {}
        for post in posts:
            created_at = post.get('created_at', '')
            if created_at:
                try:
                    # è§£ææ—¥æœŸ
                    date_obj = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = date_obj.strftime('%Y-%m-%d')
                    daily_mentions[date_str] = daily_mentions.get(date_str, 0) + 1
                except:
                    pass
        
        print(f"  ğŸ“… æ¯æ—¥æåŠæ•°: {len(daily_mentions)} å¤©")
        if daily_mentions:
            max_day = max(daily_mentions.items(), key=lambda x: x[1])
            print(f"  ğŸ”¥ æœ€æ´»è·ƒçš„ä¸€å¤©: {max_day[0]} ({max_day[1]} ä¸ªå¸–å­)")
        
        return True
        
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å†å²æ•°æ®åˆ†æåŠŸèƒ½...")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    data_ok = test_data_loading()
    
    # æµ‹è¯•æŒ‡æ ‡è®¡ç®—
    metrics_ok = test_metrics_calculation()
    
    # æµ‹è¯•è¶‹åŠ¿åˆ†æ
    trend_ok = test_trend_analysis()
    
    print("\nğŸ“‹ æµ‹è¯•ç»“æœ:")
    print(f"  ğŸ“Š æ•°æ®åŠ è½½: {'âœ…' if data_ok else 'âŒ'}")
    print(f"  ğŸ“ˆ æŒ‡æ ‡è®¡ç®—: {'âœ…' if metrics_ok else 'âŒ'}")
    print(f"  ğŸ“… è¶‹åŠ¿åˆ†æ: {'âœ…' if trend_ok else 'âŒ'}")
    
    if data_ok and metrics_ok and trend_ok:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å†å²æ•°æ®åˆ†æåŠŸèƒ½æ­£å¸¸")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥")

if __name__ == "__main__":
    main()
